{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d24a158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from Cdatasets.tokenizer import MathTokenizer, load_tokenizer\n",
    "from scripts.model import GQATransformer\n",
    "from transformers import AutoTokenizer\n",
    "from utils.binary_dataset import TokenizedBinaryDataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6dee90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BIN] Loading pre-tokenized binary: data/automathtext.bin\n",
      "============== Dataset Inspection ==============\n",
      "Total tokens in binary:  3,559,082\n",
      "Total samples available: 13,902\n",
      "Model block size:        256\n",
      "================================================\n",
      "\n",
      "--- Showing first 100 samples ---\n",
      "\n",
      "================ Sample 0 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "<s> Convergence directions of the randomized Gauss--Seidel method and its extension\n",
      "\n",
      "The randomized Gauss--Seidel method and its extension have attracted much attention recently and their convergence rates have been considered extensively. However, the convergence rates are usually determined by upper bounds, which cannot fully reflect the actual convergence. In this paper, we make a detailed analysis of their convergence behaviors. The analysis shows that the larger the singular value of $A$ is, the faster the error decays in the corresponding singular vector space, and the convergence directions are mainly driven by the large singular values at the beginning, then gradually driven by the small singular values, and finally by the smallest nonzero singular value. These results explain the phenomenon found in the extensive numerical experiments appearing in the literature that these two methods seem to converge faster at the beginning. Numerical examples are provided to confirm the above findings.\n",
      "\n",
      "\\section{Introduction}\n",
      "Linear least squares problem is a ubiquitous problem arising frequently in data analysis and scientific computing. Specifically, given a data matrix $A\\in R^{m\\times n}$ and a data vector $b\\in R^{m}$, a linear least squares problem can be written as follows\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [1, 1281, 369, 10238, 18112]\n",
      "Target IDs (first 5): [1281, 369, 10238, 18112, 310]\n",
      "\n",
      "================ Sample 1 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "\n",
      "\\begin{equation}\n",
      "\\label{ls}\n",
      "\\min \\limits _{ x \\in R^{n}}\\|b-Ax\\|^2_{2}.\n",
      "\\end{equation}\n",
      "In the literature, several direct methods have been proposed for solving its normal equations $A^TAx=A^Tb$ through either the QR factorization or the singular value decomposition (SVD) of $A^TA$ \\cite{bjorck1996numerical, Higham2002}, which can be prohibitive when the matrix is large--scale. Hence, iterative methods are considered for solving large linear least squares problem, such as the famous Gauss--Seidel method \\cite{Saad2003}.\n",
      "\n",
      "In \\cite{Leventhal2010}, Leventhal and Lewis proved that the randomized Gauss--Seidel (RGS) method, also known as the randomized coordinate descent method, converges to the solution at a linear rate in expectation. This method works on the columns of the matrix $A$ at random with probability proportional to their norms. Later, Ma, Needell and Ramdas \\cite{Ma2015} provided\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [13, 29905, 463, 29912, 2573]\n",
      "Target IDs (first 5): [29905, 463, 29912, 2573, 29913]\n",
      "\n",
      "================ Sample 2 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "a unified theory of the RGS method and the randomized Kaczmarz (RK) method \\cite{Strohmer2009}, where the latter method works on the rows of $A$, and showed that the RGS method converges to the minimum Euclidean norm least squares solution $x_{\\star}$ of (\\ref{ls}) only when the matrix $A$ is of full column rank. To further develop the RGS method for more general matrix, inspired by the randomized extended Kaczmarz (REK) method \\cite{Completion2013},\n",
      "Ma et al. \\cite{Ma2015} presented a variant of the RGS mehtod, i.e., randomized extended Gauss--Seidel (REGS) method, and proved that the REGS method converges to $x_{\\star}$ regardless of whether the matrix $A$ has full column rank. After that, many variants of the RGS (or REGS) method were developed and studied extensively; see for example \\cite{gower2015randomized, nutini2015coordinate, Hefny2017,tu2017breaking, xu\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [263, 443, 2164, 6368, 310]\n",
      "Target IDs (first 5): [443, 2164, 6368, 310, 278]\n",
      "\n",
      "================ Sample 3 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "2018hybrid,Dukui2019,razaviyayn2019linearly} and references therein.\n",
      "\n",
      "To the best of our knowledge, when studying the convergence properties of the RGS and REGS methods, people mainly pay attention to their convergence rates and usually give corresponding upper bounds, and no\n",
      "work focuses on what determines their convergence rates, what drives their convergence directions, and what their ultimate directions is.\n",
      "As we know, the obtained upper bound of convergence can only be used as a reference for the convergence rate, and cannot truly reflect the empirical convergence of the method. So it is interesting to consider the above three problems.\n",
      "\n",
      "In 2017, Jiao, Jin and Lu \\cite{jiao2017preasymptotic} analyzed the preasymptotic convergence of the RK method. Recently, Steinerberger \\cite{steinerberger2021randomized} made a more detailed analysis of the convergence property of the RK method for overdetermined full rank linear system. The author showed that the right singular vectors of the matrix $A$ describe the directions of distinguished dynamics and the RK\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [29906, 29900, 29896, 29947, 5819]\n",
      "Target IDs (first 5): [29900, 29896, 29947, 5819, 19515]\n",
      "\n",
      "================ Sample 4 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "method converges along small right singular vectors.\n",
      "After that, Zhang and Li \\cite{zhang2021preconvergence} considered the convergence property of the REK method for all types of linear systems (consistent or inconsistent, overdetermined or underdetermined, full-rank or rank-deficient) and showed that the REK method converges to the minimum Euclidean norm least squares solution $x_{\\star}$ with different decay rates in different right singular vectors spaces.\n",
      "\n",
      "In this paper, we analyze the convergence properties of the RGS and REGS methods for linear least squares problem and show that the decay rates of the sequences $\\{Ax_{k}\\}_{k=1}^{\\infty}$ and $\\{ x_{k}\\}_{k=1}^{\\infty}$ (resp., the sequences $\\{Az_{k}\\}_{k=1}^{\\infty}$ and $\\{ z_{k}\\}_{k=1}^{\\infty}$ ) generated by the RGS method (resp., the REGS method) are depend on the size of singular values of $A$. Specifically, the larger the singular value of $A$ is, the faster the error decays in the corresponding singular vector space, and the convergence directions are mainly driven by the\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [1158, 24144, 3412, 2319, 1492]\n",
      "Target IDs (first 5): [24144, 3412, 2319, 1492, 13512]\n",
      "\n",
      "================ Sample 5 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "large singular values at the beginning, then gradually driven by the small singular values, and finally by the smallest nonzero singular value.\n",
      "\n",
      "\n",
      "The rest of this paper is organized as follows. We first introduce some notations and preliminaries in Section \\ref{sec2} and then present our main results about the RGS and REGS methods in Section \\ref{sec3} and Section \\ref{sec4}, respectively. Numerical experiments are given in Section \\ref{sec5}.\n",
      "\n",
      "\n",
      "\\section{Notations and preliminaries }\\label{sec2}\n",
      "Throughout the paper, for a matrix $A$, $A^T$, $A^{(i)}$, $A_{(j)}$,\n",
      "$\\sigma_i(A)$, $\\sigma_r(A)$, $\\|A\\|_F$, and $\\mathcal{R}(A)$ denote its transpose, $i$th row (or $i$th entry in the case of a vector), $j$th column, $i$th singular value, smallest nonzero singular value, Frobenius norm, and column space, respectively. For any integer $m\\geq1$, let $[m]:=\\{1, 2\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [2919, 13512, 1819, 472, 278]\n",
      "Target IDs (first 5): [13512, 1819, 472, 278, 6763]\n",
      "\n",
      "================ Sample 6 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      ", 3, ..., m\\}$. If the matrix $G\\in R^{n\\times n}$ is positive definite, we define the energy norm of any vector $x\\in R^{n}$ as $\\| x\\|_G:=\\sqrt{x^TGx}$. In addition, we denote the identity matrix by $I$, its $j$th column by $e_{(j)}$ and the expectation of any random variable $\\xi$ by $\\mathbb{E} [\\xi]$.\n",
      "\n",
      "In the following, we use $x_{\\star}=A^{\\dag}b$ to denote the minimum Euclidean norm least squares solution of (\\ref{ls}), where $A^{\\dag}$ denotes the Moore--Penrose pseudoinverse of the matrix $A$. Because the SVD is the basic tool for the convergence analysis in next two sections, we denote the SVD \\cite{golub2013matrix} of $A\\in R^{m\\times n}$ by\n",
      "\\begin{align}\n",
      "A=U\\Sigma V^{T}, \\notag\n",
      "\\end{align}\n",
      "where $U=[u_1, u_2, \\ldots u_m]\\in R^{m\\times\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [29892, 29871, 29941, 29892, 2023]\n",
      "Target IDs (first 5): [29871, 29941, 29892, 2023, 29892]\n",
      "\n",
      "================ Sample 7 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "m}$ and $V=[v_1, v_2, \\ldots v_n]\\in R^{n\\times n}$ are column orthonormal matrices and their column vectors known as the left and right singular vectors, respectively, and $\\Sigma\\in R^{m\\times n}$ is diagonal with the diagonal elements ordered nonincreasingly, i.e., $\\sigma_1(A)\\geq \\sigma_2(A)\\geq \\ldots \\sigma_r(A)>0$ with $r\\leq \\min\\{m, n\\}$.\n",
      "\n",
      "\n",
      "\n",
      "\\section{Convergence directions of the RGS method}\\label{sec3}\n",
      "We first list the RGS method \\cite{Leventhal2010, Ma2015} in Algorithm \\ref{alg1} and restate its convergence bound in Theorem \\ref{theorem0}.\n",
      "\\begin{alg}\n",
      "\\label{alg1}\n",
      " The RGS method\n",
      "\\begin{enumerate}[\n",
      "\\item \\mbox{INPUT:} ~$A$, $b$, $\\ell$, $x_{0}\\in R^{n }$\n",
      "\\item For $k=1, 2, \\ldots, \\\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [286, 1042, 322, 395, 29963]\n",
      "Target IDs (first 5): [1042, 322, 395, 29963, 11759]\n",
      "\n",
      "================ Sample 8 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "ell-1$ do\n",
      "\\item ~~~~Select $j\\in [n]$ with probability $\\frac{\\|A_{(j)}\\|^2_2}{\\|A\\|^2_F}$\n",
      "\\item ~~~~Set $x_{k}=x_{k-1}-\\frac{A_{(j)}^T ( Ax_{k-1}-b)}{ \\| A_{(j)} \\|_{2}^{2}}e_{(j)}$\n",
      "\\item End for\n",
      "\\end{enumerate}\n",
      "\\end{alg}\n",
      "\n",
      "\\begin{thm}\n",
      "\\label{theorem0}\\cite{Leventhal2010, Ma2015}\n",
      "Let $A\\in R^{m\\times n}$, $b\\in R^{m}$, $x_{\\star}=A^{\\dag}b$ be the minimum Euclidean norm least squares solution, and $x_k$ be the $k$th approximation of the RGS method generated by Algorithm \\ref{alg1} with initial guess $x_{0}\\in R^{n }$. Then\n",
      "\\begin{align}\n",
      "\\mathbb{E}[\\| Ax_{k}- Ax_{\\star}\\|_2^2]\\leq(1-\\frac{\\\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [514, 29899, 29896, 29938, 437]\n",
      "Target IDs (first 5): [29899, 29896, 29938, 437, 13]\n",
      "\n",
      "================ Sample 9 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "sigma_r^2(A)}{\\|A\\|^2_F})^k \\|Ax_{0}- Ax_{\\star}\\|_2^2.\\label{th0}\n",
      "\\end{align}\n",
      "\\end{thm}\n",
      "\n",
      "\\begin{rmk}\n",
      "\\label{rmk1}\n",
      " Theorem \\ref{theorem0} shows that $Ax_{k}$ converges linearly in expectation to $Ax_{\\star}$ regardless of whether the matrix $A$ has full rank. Since $\\| Ax_{k}- Ax_{\\star}\\|_2^2=\\|  x_{k}- x_{\\star}\\|_{A^TA}^2$, it follows from (\\ref{th0}) that\n",
      "\\begin{align}\n",
      "\\mathbb{E}[\\| x_{k}-  x_{\\star}\\|_{A^TA}^2]\\leq(1-\\frac{\\sigma_r^2(A)}{\\|A\\|^2_F})^k \\| x_{0}-  x_{\\star}\\|_{A^TA}^2,\\notag\n",
      "\\end{align}\n",
      "which implies that $ x_{k}$ converges linearly in expectation to the minimum Euclidean norm least squares solution $ x_{\\star}$ when\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [3754, 29918, 29878, 29985, 29906]\n",
      "Target IDs (first 5): [29918, 29878, 29985, 29906, 29898]\n",
      "\n",
      "================ Sample 10 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "the matrix $A$ is overdetermined and of full column rank, but can not converge to $ x_{\\star}$ when $A$ is not full column rank. So, we\n",
      "assume that the matrix $A$ is of full column rank in this section.\n",
      "\\end{rmk}\n",
      "\n",
      "Now, we give our three main results of the RGS method.\n",
      "\\begin{thm}\n",
      "\\label{theorem1}\n",
      "Let $A\\in R^{m\\times n}$, $b\\in R^{m}$, $x_{\\star}=A^{\\dag}b$ be the minimum Euclidean norm least squares solution, and $x_k$ be the $k$th approximation of the RGS method generated by Algorithm \\ref{alg1} with initial guess $x_{0}\\in R^{n }$. Then\n",
      "\\begin{align}\n",
      "\\mathbb{E}[\\langle Ax_{k}- Ax_{\\star}, u_{\\ell} \\rangle]= (1-\\frac{\\sigma_\\ell^2(A)}{\\|A\\|^2_F})^k \\langle Ax_{0}- Ax_{\\star}, u_{\\ell} \\rangle.\\label{th1}\n",
      "\\end{align\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [278, 4636, 395, 29909, 29938]\n",
      "Target IDs (first 5): [4636, 395, 29909, 29938, 338]\n",
      "\n",
      "================ Sample 11 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "}\n",
      "\\end{thm}\n",
      "\n",
      "\\begin{pf}\n",
      "Let $\\mathbb{E}_{k-1}[\\cdot]$ be the conditional expectation conditioned on the first $k-1$ iterations of the RGS method. Then, from Algorithm \\ref{alg1}, we have\n",
      "\\begin{align}\n",
      " & \\mathbb{E}_{k-1}[\\langle Ax_{k}- Ax_{\\star}, u_{\\ell} \\rangle]\\notag\n",
      "\\\\\n",
      " &= \\sum\\limits_{j=1}^{n}\\frac{ \\|A_{ (j )} \\|_{2}^{2}}{\\|A\\|_{F}^{2}} \\langle Ax_{k-1}-\\frac{A_{ (j )}^T(Ax_{k-1}-b)}{\\|A_{ (j )} \\|_{2}^{2}}A_{ (j )}- Ax_{\\star}, u_{\\ell} \\rangle \\notag\n",
      "\\\\\n",
      " &=  \\langle Ax_{k-1}- Ax_{\\star}, u_{\\ell} \\rangle - \\frac{ 1}{\\|A\\|_{F}^{2}} \\sum\\limits_{j=1}^{n} \\langle A_{ (j )}^T(Ax_{k-\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [29913, 13, 29905, 355, 29912]\n",
      "Target IDs (first 5): [13, 29905, 355, 29912, 12743]\n",
      "\n",
      "================ Sample 12 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "1}-b) A_{ (j )} , u_{\\ell}  \\rangle  \\notag\n",
      "\\\\\n",
      " &=  \\langle Ax_{k-1}- Ax_{\\star}, u_{\\ell} \\rangle - \\frac{ 1}{\\|A\\|_{F}^{2}} \\sum\\limits_{j=1}^{n} \\langle A_{ (j )},Ax_{k-1}-b  \\rangle \\langle A_{ (j )}, u_{\\ell}  \\rangle  \\notag\n",
      "\\\\\n",
      " &=  \\langle Ax_{k-1}- Ax_{\\star}, u_{\\ell} \\rangle - \\frac{ 1}{\\|A\\|_{F}^{2}}  \\langle A^T(Ax_{k-1}-b)   , A^Tu_{\\ell}  \\rangle,  \\notag\n",
      "\\end{align}\n",
      "which together with the facts $A^T(b-Ax_{\\star})=0$ and $A^Tu_{\\ell}=\\sigma_{\\ell}(A) v_{\\ell} $ yields\n",
      "\\begin{align}\n",
      " & \\mathbb{E}_{k-1}[\\langle Ax_{k}- Ax_{\\star}, u_{\\ell} \\\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [29896, 7402, 29890, 29897, 319]\n",
      "Target IDs (first 5): [7402, 29890, 29897, 319, 648]\n",
      "\n",
      "================ Sample 13 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "rangle]\\notag\n",
      "\\\\\n",
      " &=  \\langle Ax_{k-1}- Ax_{\\star}, u_{\\ell} \\rangle - \\frac{ 1}{\\|A\\|_{F}^{2}}  \\langle A^T(Ax_{k-1}-Ax_{\\star})   , A^Tu_{\\ell}  \\rangle  \\notag\n",
      "\\\\\n",
      " &=  \\langle Ax_{k-1}- Ax_{\\star}, u_{\\ell} \\rangle - \\frac{ 1}{\\|A\\|_{F}^{2}}  \\langle A^T(\\sum\\limits_{i=1}^{m} \\langle Ax_{k-1}-Ax_{\\star}, u_i\\rangle u_i)   , A^Tu_{\\ell}  \\rangle   \\notag\n",
      "\\\\\n",
      " &=  \\langle Ax_{k-1}- Ax_{\\star}, u_{\\ell} \\rangle - \\frac{ 1}{\\|A\\|_{F}^{2}}  \\langle  (\\sum\\limits_{i=1}^{m} \\langle Ax_{k-1}-Ax_{\\star}, u_i\\rangle \\sigma_{i}(A) v_{i})   , \\sigma_{\\ell}(A)\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [5854, 10725, 1333, 351, 13]\n",
      "Target IDs (first 5): [10725, 1333, 351, 13, 1966]\n",
      "\n",
      "================ Sample 14 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "v_{\\ell}  \\rangle   \\notag\n",
      "\\\\\n",
      " &=  \\langle Ax_{k-1}- Ax_{\\star}, u_{\\ell} \\rangle - \\frac{\\sigma_{\\ell}^2(A)}{\\|A\\|_{F}^{2}}       \\langle Ax_{k-1}-Ax_{\\star}, u_{\\ell}\\rangle    \\notag\n",
      "\\\\\n",
      " &=  (1- \\frac{\\sigma_{\\ell}^2(A)}{\\|A\\|_{F}^{2}}   )   \\langle Ax_{k-1}-Ax_{\\star}, u_{\\ell}\\rangle.    \\notag\n",
      "\\end{align}\n",
      "Thus, by taking the full expectation on both sides, we have\n",
      "\\begin{align}\n",
      " \\mathbb{E}[\\langle Ax_{k}- Ax_{\\star}, u_{\\ell} \\rangle] =  (1- \\frac{\\sigma_{\\ell}^2(A)}{\\|A\\|_{F}^{2}}   )  \\mathbb{E}[ \\langle Ax_{k-1}-Ax_{\\star}, u_{\\ell}\\rangle  ] = \\ldots= (1- \\frac{\\sigma_{\\ell}^2(A)}{\\|A\\|_{F}^{2}}   )\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [325, 1665, 514, 29913, 29871]\n",
      "Target IDs (first 5): [1665, 514, 29913, 29871, 320]\n",
      "\n",
      "================ Sample 15 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "^k   \\langle Ax_{0}-Ax_{\\star}, u_{\\ell}\\rangle,   \\notag\n",
      "\\end{align}\n",
      "which is the estimate (\\ref{th1}).\n",
      "\n",
      "\\end{pf}\n",
      "\n",
      "\\begin{rmk}\n",
      "\\label{rmk2}\n",
      " Theorem \\ref{theorem1} shows that the decay rates of $\\|Ax_k-Ax_{\\star}\\|_2$ are different in different left singular vectors spaces. Specifically, the decay rates are dependent on the singular values: the larger the singular value of $A$ is, the faster the error decays in the corresponding left singular vector space. This implies that the smallest singular value will lead to the slowest rate of convergence, which is the one in (\\ref{th0}). So, the convergence bound presented in \\cite{Leventhal2010, Ma2015} is optimal.\n",
      "\\end{rmk}\n",
      "\n",
      "\\begin{rmk}\n",
      "\\label{rmk3}\n",
      "Let $r_k=b-Ax_k$ be the residual vector with respect to the $k$-th approximation $x_k$, and $r_{\\star\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [29985, 29895, 259, 320, 6990]\n",
      "Target IDs (first 5): [29895, 259, 320, 6990, 22523]\n",
      "\n",
      "================ Sample 16 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "}=b-Ax_{\\star}$ be the true residual vector with respect to the minimum Euclidean norm least squares solution $x_{\\star}$. It follows from (\\ref{th1}) and $Ax_k-Ax_{\\star}=-( r_{k}- r_{\\star})$ that\n",
      "\\begin{align}\n",
      "\\mathbb{E}[\\langle r_{k}- r_{\\star}, u_{\\ell} \\rangle]= (1-\\frac{\\sigma_\\ell^2(A)}{\\|A\\|^2_F})^k \\langle r_{0}- r_{\\star}, u_{\\ell} \\rangle.\\notag\n",
      "\\end{align}\n",
      "Hence, Theorem \\ref{theorem1} also implies that the decay rates of $ \\| r_{k}- r_{\\star}\\|_2 $ of the RGS method depend on the singular values.\n",
      "\\end{rmk}\n",
      "\n",
      "\\begin{rmk}\n",
      "\\label{rmk4}\n",
      " Using the facts $\\langle Ax_{k}- Ax_{\\star}, u_{\\ell} \\rangle=\\langle  x_{k}- x_{\\star},A^Tu_{\\ell} \\rangle$ and $A^Tu_{\\ell}=\\sigma_{\\ell}(A\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [5369, 29890, 29899, 29909, 29916]\n",
      "Target IDs (first 5): [29890, 29899, 29909, 29916, 1665]\n",
      "\n",
      "================ Sample 17 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      ") v_{\\ell} $, from (\\ref{th1}), we have\n",
      " \\begin{align}\n",
      "\\mathbb{E}[\\langle  x_{k}-  x_{\\star},  v_{\\ell}  \\rangle]= (1-\\frac{\\sigma_\\ell^2(A)}{\\|A\\|^2_F})^k \\langle  x_{0}-  x_{\\star},   v_{\\ell}  \\rangle,\\notag\n",
      "\\end{align}\n",
      "which recovers the decay rates of the RK method in different right singular vectors spaces \\cite{steinerberger2021randomized}. In this view, both RGS and RK methods are essentially equivalent.\n",
      "\\end{rmk}\n",
      "\n",
      "\\begin{thm}\n",
      "\\label{theorem2}\n",
      "Let $A\\in R^{m\\times n}$, $b\\in R^{m}$, $x_{\\star}=A^{\\dag}b$ be the minimum Euclidean norm least squares solution, and $x_k$ be the $k$th approximation of the RGS method generated by Algorithm \\ref{alg1} with initial guess $x_{0}\\in R^{n }$. Then\n",
      "\\begin{align\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [29897, 325, 1665, 514, 29913]\n",
      "Target IDs (first 5): [325, 1665, 514, 29913, 29313]\n",
      "\n",
      "================ Sample 18 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "}\n",
      "\\mathbb{E}[\\|  Ax_{k}- Ax_{\\star}\\|_2^2]= \\mathbb{E}[(1-\\frac{1}{\\|A\\|^2_F}\\|A^T\\frac{Ax_{k-1}- Ax_{\\star}}{\\|Ax_{k-1}- Ax_{\\star}\\|_2}\\|_2^2)\\|  Ax_{k-1}- Ax_{\\star}\\|_2^2]. \\notag\n",
      "\\end{align}\n",
      "\\end{thm}\n",
      "\n",
      "\\begin{pf}\n",
      "Similar to the proof of \\cite{Ma2015}, we can derive the desired result.\n",
      "\\end{pf}\n",
      "\n",
      "\\begin{rmk}\n",
      "\\label{rmk5}\n",
      "Since $\\|A^T\\frac{Ax_{k-1}- Ax_{\\star}}{\\|Ax_{k-1}- Ax_{\\star}\\|_2}\\|_2^2\\geq\\sigma_r^2(A)$, Theorem \\ref{theorem2} implies that the RGS method actually converges faster if $Ax_{k-1}-Ax_{\\star}$ is not close to left\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [29913, 13, 29905, 1995, 29912]\n",
      "Target IDs (first 5): [13, 29905, 1995, 29912, 29923]\n",
      "\n",
      "================ Sample 19 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "singular vectors corresponding to the small singular values of $A$ .\n",
      "\\end{rmk}\n",
      "\n",
      "\\begin{thm}\n",
      "\\label{theorem3}\n",
      "Let $A\\in R^{m\\times n}$, $b\\in R^{m}$, $x_{\\star}=A^{\\dag}b$ be the minimum Euclidean norm least squares solution, and $x_k$ be the $k$th approximation of the RGS method generated by Algorithm \\ref{alg1} with initial guess $x_{0}\\in R^{n }$. Then\n",
      "\\begin{align}\n",
      "\\mathbb{E}[\\langle  \\frac{Ax_{k}- Ax_{\\star}}{\\|Ax_{k}- Ax_{\\star}\\|_2}, \\frac{Ax_{k+1}- Ax_{\\star}}{\\|Ax_{k+1}- Ax_{\\star}\\|_2}  \\rangle^2]= 1 -\\frac{1}{\\|A\\|^2_F} \\mathbb{E}[ \\|A^T\\frac{Ax_{k }- Ax_{\\star}}{\\|Ax_{k }- Ax_{\\star}\\|_2}\\|_2^2  ]. \\label\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [13512, 12047, 6590, 304, 278]\n",
      "Target IDs (first 5): [12047, 6590, 304, 278, 2319]\n",
      "\n",
      "================ Sample 20 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "{th3}\n",
      "\\end{align}\n",
      "\\end{thm}\n",
      "\n",
      "\\begin{pf}\n",
      "From Algorithm \\ref{alg1}, we have\n",
      "\\begin{align}\n",
      " & \\mathbb{E}_{k}[\\langle  \\frac{Ax_{k}- Ax_{\\star}}{\\|Ax_{k}- Ax_{\\star}\\|_2}, \\frac{Ax_{k+1}- Ax_{\\star}}{\\|Ax_{k+1}- Ax_{\\star}\\|_2}  \\rangle^2]\\notag\n",
      "\\\\\n",
      " &= \\mathbb{E}_{k}[\\langle  \\frac{Ax_{k}- Ax_{\\star}}{\\|Ax_{k}- Ax_{\\star}\\|_2}, \\frac{Ax_{k}-\\frac{A_{ (j )}^T(Ax_{k}-b)}{\\|A_{ (j )} \\|_{2}^{2}}A_{ (j )}- Ax_{\\star}}{\\|Ax_{k}-\\frac{A_{ (j )}^T(Ax_{k}-b)}{\\|A_{ (j )} \\|_{2}^{2}}A_{ (j )}- Ax_{\\star}\\|_2}  \\\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [29912, 386, 29941, 29913, 13]\n",
      "Target IDs (first 5): [386, 29941, 29913, 13, 29905]\n",
      "\n",
      "================ Sample 21 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "rangle^2]\\notag\n",
      "\\\\\n",
      " &= \\mathbb{E}_{k}[\\frac{1}{\\|Ax_{k}- Ax_{\\star}\\|_2^2\\cdot \\|Ax_{k}-\\frac{A_{ (j )}^T(Ax_{k}-b)}{\\|A_{ (j )} \\|_{2}^{2}}A_{ (j )}- Ax_{\\star}\\|_2^2}\\langle   Ax_{k}- Ax_{\\star} ,  Ax_{k}-\\frac{A_{ (j )}^T(Ax_{k}-b)}{\\|A_{ (j )} \\|_{2}^{2}}A_{ (j )}- Ax_{\\star} \\rangle^2]. \\notag\n",
      "\\end{align}\n",
      "Since $\\langle   Ax_{k}- Ax_{\\star} ,  Ax_{k}-\\frac{A_{ (j )}^T(Ax_{k}-b)}{\\|A_{ (j )} \\|_{2}^{2}}A_{ (j )}- Ax_{\\star} \\rangle=\\|Ax_{k}-\\frac{A_{ (j )}^T(Ax_{k}-b)}{\\|A_{ (j )} \\|_{2\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [5854, 29985, 29906, 10725, 1333]\n",
      "Target IDs (first 5): [29985, 29906, 10725, 1333, 351]\n",
      "\n",
      "================ Sample 22 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "}^{2}}A_{ (j )}- Ax_{\\star}\\|_2^2$, we have\n",
      "\\begin{align}\n",
      " & \\mathbb{E}_{k}[\\langle  \\frac{Ax_{k}- Ax_{\\star}}{\\|Ax_{k}- Ax_{\\star}\\|_2}, \\frac{Ax_{k+1}- Ax_{\\star}}{\\|Ax_{k+1}- Ax_{\\star}\\|_2}  \\rangle^2]\\notag\n",
      "\\\\\n",
      " &= \\mathbb{E}_{k}[\\frac{ 1}{\\|Ax_{k}- Ax_{\\star}\\|_2^2 }\\|Ax_{k}-\\frac{A_{ (j )}^T(Ax_{k}-b)}{\\|A_{ (j )} \\|_{2}^{2}}A_{ (j )}- Ax_{\\star}\\|_2^2 ] \\notag\n",
      "\\\\\n",
      " &= \\mathbb{E}_{k}[\\frac{ 1}{\\|Ax_{k}- Ax_{\\star}\\|_2^2 }(\\|Ax_{k} - Ax_{\\star}\\|_2^2-2 \\langle Ax_{k}- Ax_{\\star}, \\frac{\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [2844, 29906, 930, 29909, 648]\n",
      "Target IDs (first 5): [29906, 930, 29909, 648, 313]\n",
      "\n",
      "================ Sample 23 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "A_{ (j )}^T(Ax_{k}-b)}{\\|A_{ (j )} \\|_{2}^{2}}A_{ (j )} \\rangle +\\frac{(A_{ (j )}^T(Ax_{k}-b))^2}{\\|A_{ (j )} \\|_{2}^{2}})  ] \\notag\n",
      "\\\\\n",
      " &= \\mathbb{E}_{k}[\\frac{ 1}{\\|Ax_{k}- Ax_{\\star}\\|_2^2 }(\\|Ax_{k} - Ax_{\\star}\\|_2^2-\\frac{(A_{ (j )}^T(Ax_{k}-Ax_{\\star}))^2}{\\|A_{ (j )} \\|_{2}^{2}})  ] \\notag\n",
      "\\\\\n",
      " &= \\mathbb{E}_{k}[1-\\frac{(A_{ (j )}^T  \\frac{Ax_{k}-Ax_{\\star}}{\\|Ax_{k}- Ax_{\\star}\\|_2  } )^2}{\\|A_{ (j )} \\|_{2}^{2}}  ] \\notag\n",
      "\\\\\n",
      " &= \\sum\\limits_{j=1}^{n}\\frac\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [29909, 648, 313, 29926, 1723]\n",
      "Target IDs (first 5): [648, 313, 29926, 1723, 2137]\n",
      "\n",
      "================ Sample 24 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "{ \\|A_{ (j )} \\|_{2}^{2}}{\\|A\\|_{F}^{2}} (1-\\frac{(A_{ (j )}^T  \\frac{Ax_{k}-Ax_{\\star}}{\\|Ax_{k}- Ax_{\\star}\\|_2  } )^2}{\\|A_{ (j )} \\|_{2}^{2}})   \\notag\n",
      "\\\\\n",
      " &=  1-\\frac{ 1}{\\|A\\|_{F}^{2}} \\| A^T  \\frac{Ax_{k}-Ax_{\\star}}{\\|Ax_{k}- Ax_{\\star}\\|_2  }  \\|_2^2.  \\notag\n",
      "\\end{align}\n",
      "Thus, by taking the full expectation on both sides, we obtain the desired result (\\ref{th3}).\n",
      "\\end{pf}\n",
      "\n",
      "\\begin{rmk}\n",
      "\\label{rmk6}\n",
      "Let $u$ and $v$ are two unit vectors, i.e., $\\|u\\|_2=1$ and $\\|v\\|_2=1$. We use inner quantity $\\langle u, v\\rangle^2$ to represent the angle between $u\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [29912, 12926, 29909, 648, 313]\n",
      "Target IDs (first 5): [12926, 29909, 648, 313, 29926]\n",
      "\n",
      "================ Sample 25 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "$ and $v$, and the bigger the angle is, the bigger the fluctuation becomes from $u$ to $v$. Theorem \\ref{theorem3} shows the fluctuation of two adjacent iterations. Specifically, when $\\|A^T\\frac{Ax_{k }- Ax_{\\star}}{\\|Ax_{k }- Ax_{\\star}\\|_2}\\|_2^2$ is large, the angle between $\\frac{Ax_{k}- Ax_{\\star}}{\\|Ax_{k}- Ax_{\\star}\\|_2}$ and $\\frac{Ax_{k+1}- Ax_{\\star}}{\\|Ax_{k+1}- Ax_{\\star}\\|_2}$ is large, which implies that $\\frac{Ax_{k}- Ax_{\\star}}{\\|Ax_{k}- Ax_{\\star}\\|_2}$ has a large fluctuation; when $\\|A^T\\frac{Ax_{k }- Ax_{\\star}}{\\|Ax_{k }- Ax_{\\star}\\|_2}\\|_2^2$ is small, the angle between $\\frac{Ax_{k}- Ax_{\\star}}{\\|Ax_{k}- Ax\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [29938, 322, 395, 29894, 1628]\n",
      "Target IDs (first 5): [322, 395, 29894, 1628, 322]\n",
      "\n",
      "================ Sample 26 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "_{\\star}\\|_2}$ and $\\frac{Ax_{k+1}- Ax_{\\star}}{\\|Ax_{k+1}- Ax_{\\star}\\|_2}$ is small, which implies that $\\frac{Ax_{k}- Ax_{\\star}}{\\|Ax_{k}- Ax_{\\star}\\|_2}$ has very little fluctuation.\n",
      "\n",
      "Since $\\|A^T\\frac{Ax_{k}- Ax_{\\star}}{\\|Ax_{k}- Ax_{\\star}\\|_2}\\|_2^2\\geq\\sigma_r^2(A)$, Theorem \\ref{theorem3} implies that if $Ax_{k}-Ax_{\\star}$ is mainly composed of left singular vectors corresponding to the small singular values of $A$, its direction hardly changes, which means that the RGS method finally converges along left singular vector corresponding to the small singular value of $A$.\n",
      "\n",
      "\\end{rmk}\n",
      "\n",
      "\\section{Convergence directions of the REGS method}\\label{sec4}\n",
      "Recalling Remark \\ref{rmk1}, when the matrix $A$ is not full column rank, the sequence $\\{x_{k}\\}_{k=\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [1665, 8508, 1012, 29989, 29918]\n",
      "Target IDs (first 5): [8508, 1012, 29989, 29918, 29906]\n",
      "\n",
      "================ Sample 27 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "1}^{\\infty}$ generated by the RGS method does not converge to the minimum Euclidean norm least squares solution $x_{\\star}$, even though $Ax_{k}$ does converge to $Ax_{\\star}$. In \\cite{Ma2015}, Ma et al. proposed an extended variant of the RGS method, i.e., the REGS method, to allow for convergence to $x_{\\star}$ regardless of whether $A$ has full column rank or not.\n",
      "\n",
      "Now, we list the REGS method presented in \\cite{ Dukui2019} in Algorithm \\ref{alg2}, which is a equivalent variant of the original REGS method \\cite{Ma2015}, and restate its convergence bound presented in \\cite{Dukui2019} in Theorem \\ref{theorem5}. From the algorithm we find that, in each iteration, $x_k$ is the $k$th approximation of the RGS method and $z_k$ is a one-step RK update for the linear system $Az=Ax_{k}$ from $z_{k-1}$.\n",
      "\n",
      "\\begin{alg}\n",
      "\\label{alg2}\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [29896, 6292, 3411, 1042, 5759]\n",
      "Target IDs (first 5): [6292, 3411, 1042, 5759, 491]\n",
      "\n",
      "================ Sample 28 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "\n",
      " The REGS method\n",
      "\\begin{enumerate}[\n",
      "\\item \\mbox{INPUT:} ~$A$, $b$, $\\ell$, $x_{0}\\in R^{n }$, $z_{0}\\in \\mathcal{R}(A^T)$\n",
      "\\item For $k=1, 2, \\ldots, \\ell-1$ do\n",
      "\\item ~~~~Select $j\\in [n]$ with probability $\\frac{\\|A_{(j)}\\|^2_2}{\\|A\\|^2_F}$\n",
      "\\item ~~~~Set $x_{k}=x_{k-1}-\\frac{A_{(j)}^T ( Ax_{k-1}-b)}{ \\| A_{(j)} \\|_{2}^{2}}e_{(j)}$\n",
      "\\item ~~~~Select $i\\in [m]$ with probability $\\frac{\\|A^{(i)}\\|^2_2}{\\|A\\|^2_F}$\n",
      "\\item ~~~~Set $z_{k}=z_{k-1}-\\frac{A^{(i)} ( z_{k-1}-x_{k})}{ \\| A^{(i)} \\|_{2}^{2}}(A^{(i)})^T$\n",
      "\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [13, 450, 5195, 10749, 1158]\n",
      "Target IDs (first 5): [450, 5195, 10749, 1158, 13]\n",
      "\n",
      "================ Sample 29 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "\\item End for\n",
      "\\end{enumerate}\n",
      "\\end{alg}\n",
      "\n",
      "\\begin{thm}\n",
      "\\label{theorem5}\\cite{Dukui2019}\n",
      " Let $A\\in R^{m\\times n}$, $b\\in R^{m}$, $x_{\\star}=A^{\\dag}b$ be the minimum Euclidean norm least squares solution, and $z_k$ be the $k$th approximation of the REGS method generated by Algorithm \\ref{alg2} with initial $x_{0}\\in R^{n }$ and $z_o\\in \\mathcal{R}(A^T)$. Then\n",
      "\\begin{align}\n",
      "\\mathbb{E}[\\| z_{k}- x_{\\star}\\|_2^2]\\leq(1-\\frac{\\sigma_r^2(A)}{\\|A\\|^2_F})^k \\|z_{0}- x_{\\star}\\|_2^2+\\frac{k}{\\|A\\|_F^2}(1-\\frac{\\sigma_r^2(A)}{\\|A\\|^2_F})^k \\|Ax_0-Ax_{\\star}\\|_2^2.\\label{\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [29905, 667, 2796, 363, 13]\n",
      "Target IDs (first 5): [667, 2796, 363, 13, 29905]\n",
      "\n",
      "================ Sample 30 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "th5}\n",
      "\\end{align}\n",
      "\\end{thm}\n",
      "\n",
      "For the REGS method, we first discuss the convergence behavior of $z_{k}- x_{\\star}$ in Theorem \\ref{theorem6} and Theorem \\ref{theorem7}, and then consider its convergence behavior of $Az_{k}- Ax_{\\star}$ in Theorem \\ref{theorem8}.\n",
      "\n",
      "\n",
      "\\begin{thm}\n",
      "\\label{theorem6}\n",
      " Let $A\\in R^{m\\times n}$, $b\\in R^{m}$, $x_{\\star}=A^{\\dag}b$ be the minimum Euclidean norm least squares solution, and $z_k$ be the $k$th approximation of the REGS method generated by Algorithm \\ref{alg2} with initial $x_{0}\\in R^{n }$ and $z_o\\in \\mathcal{R}(A^T)$. Then\n",
      "\\begin{align}\n",
      "\\mathbb{E}[\\langle z_{k}- x_{\\star}, v_{\\ell} \\rangle]= (1 -\\frac{\\sigma_{\\ell}^2(A)}{\\|A\\|^2_F} )^k    \\langle z_{0}-x_{\\star\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [386, 29945, 29913, 13, 29905]\n",
      "Target IDs (first 5): [29945, 29913, 13, 29905, 355]\n",
      "\n",
      "================ Sample 31 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "}, v_{\\ell}\\rangle  +\\frac{k}{\\|A\\|^2_F} (1-\\frac{\\sigma_\\ell^2(A)}{\\|A\\|^2_F})^k \\langle A^T(Ax_{0}- Ax_{\\star}),      v_{\\ell} \\rangle .\\label{th6}\n",
      "\\end{align}\n",
      "\\end{thm}\n",
      "\\begin{pf}\n",
      "From Algorithm \\ref{alg2},\n",
      "we have\n",
      "\\begin{align}\n",
      "&\\mathbb{E}[\\langle z_{k}- x_{\\star}, v_{\\ell} \\rangle] \\notag\n",
      " \\\\\n",
      "&= \\mathbb{E}[\\langle z_{k-1}-\\frac{A^{(i)} ( z_{k-1}-x_{k})}{ \\| A^{(i)} \\|_{2}^{2}}(A^{(i)})^T- x_{\\star}, v_{\\ell} \\rangle] \\notag\n",
      " \\\\\n",
      "&= \\mathbb{E}[\\langle (I-\\frac{(A^{(i)})^TA^{(i)}}{\\| A^{(i)} \\|_{2}^{2}})( z_{k-1}-x_{\\star}), v_{\\ell}  \\rangle]\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [1118, 325, 1665, 514, 1012]\n",
      "Target IDs (first 5): [325, 1665, 514, 1012, 5854]\n",
      "\n",
      "================ Sample 32 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "+\\mathbb{E}[\\langle \\frac{(A^{(i)})^TA^{(i)}}{\\| A^{(i)} \\|_{2}^{2}}( x_{k}-x_{\\star}),  v_{\\ell}  \\rangle],  \\label{th65}\n",
      "\\end{align}\n",
      "so we next consider $\\mathbb{E}[\\langle (I-\\frac{(A^{(i)})^TA^{(i)}}{\\| A^{(i)} \\|_{2}^{2}})( z_{k-1}-x_{\\star}), v_{\\ell}  \\rangle]$ and $\\mathbb{E}[\\langle \\frac{(A^{(i)})^TA^{(i)}}{\\| A^{(i)} \\|_{2}^{2}}( x_{k}-x_{\\star}),  v_{\\ell}  \\rangle]$ separately.\n",
      "\n",
      "We first consider$\\mathbb{E}[\\langle (I-\\frac{(A^{(i)})^TA^{(i)}}{\\| A^{(i)} \\|_{2}^{2}})( z_{k-1}-x_{\\star}), v_{\\ell}  \\rangle]$. Let $\\mathbb{E}_{k-1}[\\cdot]$ be the conditional expectation conditioned on the first $k-1$ iterations of\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [17501, 1995, 29912, 29923, 4400]\n",
      "Target IDs (first 5): [1995, 29912, 29923, 4400, 29905]\n",
      "\n",
      "================ Sample 33 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "the REGS method. That is,\n",
      "\\begin{align}\n",
      " \\mathbb{E}_{k-1}[\\cdot]= \\mathbb{E}[\\cdot|j_1, i_1, j_2, i_2, \\ldots, j_{k-1}, i_{k-1}],  \\notag\n",
      "\\end{align}\n",
      "where $j_{t^*}$ is the ${t^*}$th column chosen and $i_{t^*}$ is the ${t^*}$th row chosen. We denote the conditional expectation conditioned on\n",
      "the first $k-1$ iterations and the $k$th column chosen as\n",
      "\\begin{align}\n",
      " \\mathbb{E}_{k-1}^{i}[\\cdot]= \\mathbb{E}[\\cdot|j_1, i_1, j_2, i_2, \\ldots, j_{k-1}, i_{k-1}, j_k]. \\notag\n",
      "\\end{align}\n",
      "Similarly, we denote the conditional expectation conditioned on the first $k-1$ iterations and the $k$th row chosen as\n",
      "\\begin{align}\n",
      " \\mathbb{E}_{k-1}^{\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [278, 5195, 10749, 1158, 29889]\n",
      "Target IDs (first 5): [5195, 10749, 1158, 29889, 2193]\n",
      "\n",
      "================ Sample 34 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "j}[\\cdot]= \\mathbb{E}[\\cdot|j_1, i_1, j_2, i_2, \\ldots, j_{k-1}, i_{k-1}, i_k]. \\notag\n",
      "\\end{align}\n",
      "Then, by the law of total expectation, we have\n",
      "\\begin{align}\n",
      " \\mathbb{E}_{k-1}[\\cdot]=  \\mathbb{E}_{k-1}^{j}[ \\mathbb{E}_{k-1}^{i}[\\cdot] ]. \\notag\n",
      "\\end{align}\n",
      "Thus, we obtain\n",
      "\\begin{align}\n",
      "&\\mathbb{E}_{k-1}[\\langle (I-\\frac{(A^{(i)})^TA^{(i)}}{\\| A^{(i)} \\|_{2}^{2}})( z_{k-1}-x_{\\star}), v_{\\ell}  \\rangle] \\notag\n",
      " \\\\\n",
      "&=\\mathbb{E}_{k-1}[\\langle z_{k-1}-x_{\\star}, v_{\\ell}  \\rangle -\\langle \\frac{A^{(i)} ( z_{k-1}-x_{\\star})}{\\| A^{(i)} \\|\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [29926, 4400, 29905, 3822, 13192]\n",
      "Target IDs (first 5): [4400, 29905, 3822, 13192, 320]\n",
      "\n",
      "================ Sample 35 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "_{2}^{2}}(A^{(i)})^T, v_{\\ell}  \\rangle] \\notag\n",
      " \\\\\n",
      "&=\\langle z_{k-1}-x_{\\star}, v_{\\ell}  \\rangle -\\frac{1}{\\|A\\|^2_F} \\sum\\limits_{i=1}^{m}\\langle A^{(i)} ( z_{k-1}-x_{\\star})(A^{(i)})^T, v_{\\ell}  \\rangle  \\notag\n",
      " \\\\\n",
      "&=\\langle z_{k-1}-x_{\\star}, v_{\\ell}  \\rangle -\\frac{1}{\\|A\\|^2_F} \\sum\\limits_{i=1}^{m}\\langle(A^{(i)})^T,  z_{k-1}-x_{\\star} \\rangle\\langle (A^{(i)})^T, v_{\\ell}  \\rangle  \\notag\n",
      " \\\\\n",
      "&=\\langle z_{k-1}-x_{\\star}, v_{\\ell}  \\rangle -\\frac{1}{\\|A\\|^2_F}  \\langle A ( z_{k-1}-x_{\\star}),A v_{\\ell}  \\rangle. \n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [648, 29906, 2844, 29906, 12156]\n",
      "Target IDs (first 5): [29906, 2844, 29906, 12156, 29909]\n",
      "\n",
      "================ Sample 36 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "\\notag\n",
      "\\end{align}\n",
      "Further, by making use of $z_{k-1}-x_{\\star}=\\sum\\limits_{i=1}^{n}\\langle z_{k-1}-x_{\\star}, v_i\\rangle v_i$ and $Av_i=\\sigma_i(A) u_i $, we get\n",
      "\\begin{align}\n",
      "&\\mathbb{E}_{k-1}[\\langle (I-\\frac{(A^{(i)})^TA^{(i)}}{\\| A^{(i)} \\|_{2}^{2}})( z_{k-1}-x_{\\star}), v_{\\ell}  \\rangle] \\notag\n",
      "\\\\\n",
      "&=\\langle z_{k-1}-x_{\\star}, v_{\\ell}  \\rangle -\\frac{1}{\\|A\\|^2_F}  \\langle A  \\sum\\limits_{i=1}^{n}\\langle z_{k-1}-x_{\\star}, v_i\\rangle v_i,\\sigma_{\\ell}(A) u_{\\ell} \\rangle   \\notag\n",
      "\\\\\n",
      "&=\\langle z_{k-1}-x_{\\star}, v_{\\ell}  \\rangle -\\frac\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [320, 1333, 351, 13, 29905]\n",
      "Target IDs (first 5): [1333, 351, 13, 29905, 355]\n",
      "\n",
      "================ Sample 37 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "{1}{\\|A\\|^2_F}  \\langle   \\sum\\limits_{i=1}^{n}\\langle z_{k-1}-x_{\\star}, v_i\\rangle \\sigma_i(A) u_i,\\sigma_{\\ell}(A) u_{\\ell} \\rangle,  \\notag\n",
      "\\end{align}\n",
      "which together with the orthogonality of the left singular vectors $u_i$ yields\n",
      "\\begin{align}\n",
      "&\\mathbb{E}_{k-1}[\\langle (I-\\frac{(A^{(i)})^TA^{(i)}}{\\| A^{(i)} \\|_{2}^{2}})( z_{k-1}-x_{\\star}), v_{\\ell}  \\rangle] \\notag\n",
      "\\\\\n",
      "&=\\langle z_{k-1}-x_{\\star}, v_{\\ell}  \\rangle -\\frac{\\sigma_{\\ell}^2(A)}{\\|A\\|^2_F}    \\langle z_{k-1}-x_{\\star}, v_{\\ell}\\rangle   \\notag\n",
      "\\\\\n",
      "&=(1 -\\frac{\\sigma_{\\ell}^2(A)}{\\|A\\|^2_F} )   \\langle z\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [29912, 29896, 3331, 29989, 29909]\n",
      "Target IDs (first 5): [29896, 3331, 29989, 29909, 7893]\n",
      "\n",
      "================ Sample 38 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "_{k-1}-x_{\\star}, v_{\\ell}\\rangle .  \\notag\n",
      "\\end{align}\n",
      "As a result, by taking the full expectation on both sides, we have\n",
      "\\begin{align}\n",
      " \\mathbb{E}[\\langle (I-\\frac{(A^{(i)})^TA^{(i)}}{\\| A^{(i)} \\|_{2}^{2}})( z_{k-1}-x_{\\star}), v_{\\ell}  \\rangle]=(1 -\\frac{\\sigma_{\\ell}^2(A)}{\\|A\\|^2_F} ) \\mathbb{E}[  \\langle z_{k-1}-x_{\\star}, v_{\\ell}\\rangle ].  \\label{th62}\n",
      "\\end{align}\n",
      "\n",
      "We now consider $\\mathbb{E}[\\langle \\frac{(A^{(i)})^TA^{(i)}}{\\| A^{(i)} \\|_{2}^{2}}( x_{k}-x_{\\star}),  v_{\\ell}  \\rangle]$. It follows from\n",
      "\\begin{align}\n",
      "&\\mathbb{E}_{k-1}[\\langle \\frac{(A^{(i)})^TA^{(i)}}{\\| A^{(i)} \\|_{2\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [648, 29895, 29899, 29896, 7402]\n",
      "Target IDs (first 5): [29895, 29899, 29896, 7402, 29916]\n",
      "\n",
      "================ Sample 39 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "}^{2}}( x_{k}-x_{\\star}),  v_{\\ell}  \\rangle] \\notag\n",
      "\\\\\n",
      "&= \\mathbb{E}_{k-1}^{j}[ \\mathbb{E}_{k-1}^{i} [ \\langle \\frac{(A^{(i)})^TA^{(i)}}{\\| A^{(i)} \\|_{2}^{2}}( x_{k}-x_{\\star}),  v_{\\ell}  \\rangle]]\\notag\n",
      "\\\\\n",
      "&= \\mathbb{E}_{k-1}^{j}[\\frac{1}{\\|A\\|^2_F} \\sum\\limits_{i=1}^{m} \\langle (A^{(i)})^TA^{(i)}( x_{k}-x_{\\star}),  v_{\\ell}  \\rangle] \\notag\n",
      "\\\\\n",
      "&= \\mathbb{E}_{k-1}^{j}[\\frac{1}{\\|A\\|^2_F} \\sum\\limits_{i=1}^{m} \\langle  A^{(i)}( x_{k}-x_{\\star}),  A^{(i)} v_{\\ell}  \\rangle] \\notag\n",
      "\\\\\n",
      "&= \\mathbb{E}_{k-1} [\\frac{1}{\\|\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [2844, 29906, 12156, 921, 648]\n",
      "Target IDs (first 5): [29906, 12156, 921, 648, 29895]\n",
      "\n",
      "================ Sample 40 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "A\\|^2_F}   \\langle  A ( x_{k}-x_{\\star}),  A   v_{\\ell}  \\rangle], \\notag\n",
      "\\end{align}\n",
      "that\n",
      "\\begin{align}\n",
      " \\mathbb{E} [\\langle \\frac{(A^{(i)})^TA^{(i)}}{\\| A^{(i)} \\|_{2}^{2}}( x_{k}-x_{\\star}),  v_{\\ell}  \\rangle]=\\mathbb{E}  [\\frac{1}{\\|A\\|^2_F}   \\langle  A ( x_{k}-x_{\\star}),  A   v_{\\ell}  \\rangle].\\label{th63}\n",
      "\\end{align}\n",
      "Since\n",
      "\\begin{align}\n",
      " \\mathbb{E}  [\\frac{1}{\\|A\\|^2_F}   \\langle  A ( x_{k}-x_{\\star}),  A   v_{\\ell}  \\rangle]=\\frac{\\sigma_{\\ell} (A)}{\\|A\\|^2_F} \\mathbb{E}  [  \\langle  A ( x_{k}-x_{\\star}), u_{\\ell}  \\rangle], \\\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [29909, 7893, 29985, 29906, 29918]\n",
      "Target IDs (first 5): [7893, 29985, 29906, 29918, 29943]\n",
      "\n",
      "================ Sample 41 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "notag\n",
      "\\end{align}\n",
      "by exploiting (\\ref{th1}) in Theorem \\ref{theorem1}, we get\n",
      "\\begin{align}\n",
      "&\\mathbb{E}  [\\frac{1}{\\|A\\|^2_F}   \\langle  A ( x_{k}-x_{\\star}),  A   v_{\\ell}  \\rangle] \\notag\n",
      "\\\\\n",
      "&=\\frac{\\sigma_{\\ell} (A)}{\\|A\\|^2_F} (1-\\frac{\\sigma_\\ell^2(A)}{\\|A\\|^2_F})^k \\langle Ax_{0}- Ax_{\\star}, u_{\\ell} \\rangle \\notag\n",
      "\\\\\n",
      "&=\\frac{1}{\\|A\\|^2_F} (1-\\frac{\\sigma_\\ell^2(A)}{\\|A\\|^2_F})^k \\langle Ax_{0}- Ax_{\\star},  A   v_{\\ell} \\rangle .\\notag\n",
      "\\end{align}\n",
      "Thus, substituting the above equality into (\\ref{th63}), we have\n",
      "\\begin{align}\n",
      " \\mathbb{E} [\\langle \\frac{(A^{(i)})^TA\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [1333, 351, 13, 29905, 355]\n",
      "Target IDs (first 5): [351, 13, 29905, 355, 29912]\n",
      "\n",
      "================ Sample 42 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "^{(i)}}{\\| A^{(i)} \\|_{2}^{2}}( x_{k}-x_{\\star}),  v_{\\ell}  \\rangle]\n",
      " &=\\frac{1}{\\|A\\|^2_F} (1-\\frac{\\sigma_\\ell^2(A)}{\\|A\\|^2_F})^k \\langle Ax_{0}- Ax_{\\star},  A   v_{\\ell} \\rangle  \\notag\n",
      " \\\\\n",
      " &=\\frac{1}{\\|A\\|^2_F} (1-\\frac{\\sigma_\\ell^2(A)}{\\|A\\|^2_F})^k \\langle A^T(Ax_{0}- Ax_{\\star}),      v_{\\ell} \\rangle . \\label{th64}\n",
      "\\end{align}\n",
      "\n",
      "Combining (\\ref{th65}), (\\ref{th62}) and (\\ref{th64}) yields\n",
      "\\begin{align}\n",
      "&\\mathbb{E}[\\langle z_{k}- x_{\\star}, v_{\\ell} \\rangle] \\notag\n",
      " \\\\\n",
      "&= \\mathbb{E}[\\langle (I-\\frac{(A^{(i)})^TA^{(i)}}{\\| A^{(i)}\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [7650, 29875, 29897, 12431, 29989]\n",
      "Target IDs (first 5): [29875, 29897, 12431, 29989, 319]\n",
      "\n",
      "================ Sample 43 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "\\|_{2}^{2}})( z_{k-1}-x_{\\star}), v_{\\ell}  \\rangle] +\\mathbb{E}[\\langle \\frac{(A^{(i)})^TA^{(i)}}{\\| A^{(i)} \\|_{2}^{2}}( x_{k}-x_{\\star}),  v_{\\ell}  \\rangle] \\notag\n",
      " \\\\\n",
      "&=(1 -\\frac{\\sigma_{\\ell}^2(A)}{\\|A\\|^2_F} ) \\mathbb{E}[  \\langle z_{k-1}-x_{\\star}, v_{\\ell}\\rangle ]+\\frac{1}{\\|A\\|^2_F} (1-\\frac{\\sigma_\\ell^2(A)}{\\|A\\|^2_F})^k \\langle A^T(Ax_{0}- Ax_{\\star}),      v_{\\ell} \\rangle \\notag\n",
      " \\\\\n",
      "&=(1 -\\frac{\\sigma_{\\ell}^2(A)}{\\|A\\|^2_F} )^2 \\mathbb{E}[  \\langle z_{k-2}-x_{\\star}, v_{\\ell}\\rangle ]+\\frac{2}{\\|A\\|^2_F} (1-\\frac{\\\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [12926, 648, 29906, 2844, 29906]\n",
      "Target IDs (first 5): [648, 29906, 2844, 29906, 930]\n",
      "\n",
      "================ Sample 44 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "sigma_\\ell^2(A)}{\\|A\\|^2_F})^k \\langle A^T(Ax_{0}- Ax_{\\star}),      v_{\\ell} \\rangle \\notag\n",
      " \\\\\n",
      "&=\\ldots=(1 -\\frac{\\sigma_{\\ell}^2(A)}{\\|A\\|^2_F} )^k   \\langle z_{0}-x_{\\star}, v_{\\ell}\\rangle  +\\frac{k}{\\|A\\|^2_F} (1-\\frac{\\sigma_\\ell^2(A)}{\\|A\\|^2_F})^k \\langle A^T(Ax_{0}- Ax_{\\star}),      v_{\\ell} \\rangle, \\notag\n",
      "\\end{align}\n",
      "which is the desired result (\\ref{th6}).\n",
      "\n",
      "\\end{pf}\n",
      "\\begin{rmk}\n",
      "\\label{rmk61}\n",
      " Theorem \\ref{theorem6} shows that the decay rates of $\\|z_k-x_{\\star}\\|_2$ are different in different right singular vectors spaces and the smallest singular value will lead to the slowest rate of convergence, which is the one in (\\ref{th5}). So, the\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [3754, 3187, 514, 29985, 29906]\n",
      "Target IDs (first 5): [3187, 514, 29985, 29906, 29898]\n",
      "\n",
      "================ Sample 45 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "convergence bound presented by Du \\cite{ Dukui2019} is optimal.\n",
      "\n",
      "\\end{rmk}\n",
      "\n",
      "\\begin{thm}\n",
      "\\label{theorem7}\n",
      " Let $A\\in R^{m\\times n}$, $b\\in R^{m}$, $x_{\\star}=A^{\\dag}b$ be the minimum Euclidean norm least squares solution, and $z_k$ be the $k$th approximation of the REGS method generated by Algorithm \\ref{alg2} with initial $x_{0}\\in R^{n }$ and $z_o\\in \\mathcal{R}(A^T)$. Then\n",
      "\\begin{align}\n",
      "\\mathbb{E}[ \\|z_{k}- x_{\\star}\\|^2_2]\\leq \\mathbb{E}[(1-\\frac{1}{\\|A\\|^2_F}\\|A\\frac{z_{k-1}-x_{\\star}}{\\|z_{k-1}-x_{\\star}\\|_2}\\|_2^2)\\|z_{k-1}-x_{\\star}\\|_2^2]+ \\frac{1}{\\|A\\|^2_F}(1-\\frac\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [17221, 3216, 9132, 491, 5334]\n",
      "Target IDs (first 5): [3216, 9132, 491, 5334, 320]\n",
      "\n",
      "================ Sample 46 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "{\\sigma_r^2(A)}{\\|A\\|^2_F})^k \\| Ax_{0}-Ax_{\\star} \\|^2_2. \\label{th7}\n",
      "\\end{align}\n",
      "\\end{thm}\n",
      "\n",
      "\\begin{pf}\n",
      "Following an analogous argument to Theorem 4 of \\cite{Dukui2019}, we get\n",
      "\\begin{align}\n",
      "\\mathbb{E}   [\\|z_{k}- x_{\\star}\\|_2^2 ]&= \\mathbb{E}[ \\|(I-\\frac{(A^{(i)})^TA^{(i)}}{\\| A^{(i)} \\|_{2}^{2}})( z_{k-1}-x_{\\star})\\|_2^2]+\\mathbb{E} [\\|  \\frac{(A^{(i)})^TA^{(i)}}{\\| A^{(i)} \\|_{2}^{2}}( x_{k}-x_{\\star})\\|_2^2], \\notag\n",
      "\\end{align}\n",
      "\\begin{align}\n",
      "\\mathbb{E}[ \\|(I-\\frac{(A^{(i)})^TA^{(i)}}{\\| A^{(i)} \\|_{2}^{\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [741, 3754, 29918, 29878, 29985]\n",
      "Target IDs (first 5): [3754, 29918, 29878, 29985, 29906]\n",
      "\n",
      "================ Sample 47 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "2}})( z_{k-1}-x_{\\star})\\|_2^2]\n",
      "&=\\mathbb{E} [(z_{k-1}-x_{\\star})^T(I-\\frac{A^TA}{\\|A\\|_F^2})(z_{k-1}-x_{\\star})]\\notag\n",
      "\\\\\n",
      "&=\\mathbb{E}[(\\|z_{k-1}-x_{\\star}\\|_2^2- \\frac{1}{\\|A\\|^2_F}\\|A(z_{k-1}-x_{\\star})\\|_2^2)]\\notag\n",
      "\\\\\n",
      "&=\\mathbb{E}[(1-\\frac{1}{\\|A\\|^2_F}\\|A\\frac{z_{k-1}-x_{\\star}}{\\|z_{k-1}-x_{\\star}\\|_2}\\|_2^2)\\|z_{k-1}-x_{\\star}\\|_2^2],\\notag\n",
      "\\end{align}\n",
      "and\n",
      "\\begin{align}\n",
      "\\mathbb{E} [\\|  \\frac{(A^{(i)})^TA^{(i)}}{\\| A^{(i)} \\|_{2}^{\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [29906, 930, 5033, 503, 648]\n",
      "Target IDs (first 5): [930, 5033, 503, 648, 29895]\n",
      "\n",
      "================ Sample 48 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "2}}( x_{k}-x_{\\star})\\|_2^2]\\leq\\frac{1}{\\|A\\|^2_F}(1-\\frac{\\sigma_r^2(A)}{\\|A\\|^2_F})^k \\| Ax_{0}-Ax_{\\star} \\|^2_2. \\notag\n",
      "\\end{align}\n",
      "Combining the above three equations, we have\n",
      "\\begin{align}\n",
      "\\mathbb{E}  [ \\|z_{k}- x_{\\star}\\|_2^2]\n",
      "\\leq\\mathbb{E}[(1-\\frac{1}{\\|A\\|^2_F}\\|A\\frac{z_{k-1}-x_{\\star}}{\\|z_{k-1}-x_{\\star}\\|_2}\\|_2^2)\\|z_{k-1}-x_{\\star}\\|_2^2]+ \\frac{1}{\\|A\\|^2_F}(1-\\frac{\\sigma_r^2(A)}{\\|A\\|^2_F})^k \\| Ax_{0}-Ax_{\\star} \\|^2_2, \\notag\n",
      "\\end{align}\n",
      "which implies the desired\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [29906, 12156, 921, 648, 29895]\n",
      "Target IDs (first 5): [12156, 921, 648, 29895, 7402]\n",
      "\n",
      "================ Sample 49 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "result (\\ref{th7}).\n",
      "\\end{pf}\n",
      "\n",
      "\\begin{rmk}\n",
      "\\label{rmk71}\n",
      "Since $\\|A\\frac{z_{k-1}-x_{\\star}}{\\|z_{k-1}-x_{\\star}\\|_2}\\|_2^2\\geq\\sigma_r^2(A)$, Theorem \\ref{theorem7} implies that $z_{k}$ of the REGS method actually converges faster if $z_{k-1}-x_{\\star}$ is not close to right singular vectors corresponding to the small singular values of $A$ .\n",
      "\n",
      "\\end{rmk}\n",
      "\n",
      "\\begin{thm}\n",
      "\\label{theorem8}\n",
      " Let $A\\in R^{m\\times n}$, $b\\in R^{m}$, $x_{\\star}=A^{\\dag}b$ be the minimum Euclidean norm least squares solution, and $z_k$ be the $k$th approximation of the REGS method generated by Algorithm \\ref{alg2} with initial $x_{0}\\in R^{n }$ and $z_o\\in \\mathcal{R}(A^T)$. Then\n",
      "\\begin{\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [1121, 3441, 999, 29912, 386]\n",
      "Target IDs (first 5): [3441, 999, 29912, 386, 29955]\n",
      "\n",
      "================ Sample 50 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "align}\n",
      "\\mathbb{E}[\\langle A z_{k}- Ax_{\\star}, u_{\\ell} \\rangle]=(1 -\\frac{\\sigma_{\\ell}^2(A)}{\\|A\\|^2_F} )^k    \\langle Az_{0}-Ax_{\\star}, v_{\\ell}\\rangle  +\\frac{k}{\\|A\\|^2_F} (1-\\frac{\\sigma_\\ell^2(A)}{\\|A\\|^2_F})^k \\langle AA^T(Ax_{0}- Ax_{\\star}),      u_{\\ell} \\rangle  .\\label{th8}\n",
      "\\end{align}\n",
      "\\end{thm}\n",
      "\n",
      "\\begin{pf}\n",
      "Similar to the proof of (\\ref{th65}) in Theorem \\ref{theorem6}, we obtain\n",
      "\\begin{align}\n",
      "\\mathbb{E}[\\langle Az_{k}- Ax_{\\star}, u_{\\ell} \\rangle]\n",
      " = \\mathbb{E}[\\langle A(I-\\frac{(A^{(i)})^TA^{(i)}}{\\| A^{(i)} \\|_{2}^{2}})( z_{k-1}-x_{\\star}), u_{\\ell}\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [2520, 29913, 13, 29905, 1995]\n",
      "Target IDs (first 5): [29913, 13, 29905, 1995, 29912]\n",
      "\n",
      "================ Sample 51 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      " \\rangle] +\\mathbb{E}[\\langle A \\frac{(A^{(i)})^TA^{(i)}}{\\| A^{(i)} \\|_{2}^{2}}( x_{k}-x_{\\star}),  u_{\\ell}  \\rangle].  \\label{th80}\n",
      "\\end{align}\n",
      "Then, we consider $\\mathbb{E}[\\langle A(I-\\frac{(A^{(i)})^TA^{(i)}}{\\| A^{(i)} \\|_{2}^{2}})( z_{k-1}-x_{\\star}), u_{\\ell} \\rangle]$ and $\\mathbb{E}[\\langle  A \\frac{(A^{(i)})^TA^{(i)}}{\\| A^{(i)} \\|_{2}^{2}}( x_{k}-x_{\\star}),  u_{\\ell}   \\rangle]$ separately.\n",
      "\n",
      "We first consider $\\mathbb{E}[\\langle A(I-\\frac{(A^{(i)})^TA^{(i)}}{\\| A^{(i)} \\|_{2}^{2}})( z_{k-1}-x_{\\star}), u_{\\ell} \\rangle]$. It follows from\n",
      "$$ \\langle A(I-\\frac{(A^{(i)})^TA^{(i)\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [29871, 320, 5854, 29962, 17501]\n",
      "Target IDs (first 5): [320, 5854, 29962, 17501, 1995]\n",
      "\n",
      "================ Sample 52 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "}}{\\| A^{(i)} \\|_{2}^{2}})( z_{k-1}-x_{\\star}), u_{\\ell} \\rangle = \\langle (I-\\frac{(A^{(i)})^TA^{(i)}}{\\| A^{(i)} \\|_{2}^{2}})( z_{k-1}-x_{\\star}), A^Tu_{\\ell} \\rangle $$ and $A^Tu_{\\ell}=\\sigma_{\\ell}(A)v_{\\ell}$, that\n",
      "\\begin{align}\n",
      "  \\mathbb{E}[\\langle A(I-\\frac{(A^{(i)})^TA^{(i)}}{\\| A^{(i)} \\|_{2}^{2}})( z_{k-1}-x_{\\star}), u_{\\ell} \\rangle] = \\sigma_{\\ell}(A)\\mathbb{E}[\\langle  (I-\\frac{(A^{(i)})^TA^{(i)}}{\\| A^{(i)} \\|_{2}^{2}})( z_{k-1}-x_{\\star}),  v_{\\ell} \\rangle],  \\notag\n",
      "\\end{align}\n",
      "which together with (\\ref{th62}), yields\n",
      "\\begin{align}\n",
      "  \\mathbb{E}[\\langle A(\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [12431, 29989, 319, 7650, 29875]\n",
      "Target IDs (first 5): [29989, 319, 7650, 29875, 2915]\n",
      "\n",
      "================ Sample 53 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "I-\\frac{(A^{(i)})^TA^{(i)}}{\\| A^{(i)} \\|_{2}^{2}})( z_{k-1}-x_{\\star}), u_{\\ell} \\rangle]\n",
      "  &= \\sigma_{\\ell}(A)(1 -\\frac{\\sigma_{\\ell}^2(A)}{\\|A\\|^2_F} ) \\mathbb{E}[  \\langle z_{k-1}-x_{\\star}, v_{\\ell}\\rangle ]  \\notag\n",
      "  \\\\\n",
      "  &=(1 -\\frac{\\sigma_{\\ell}^2(A)}{\\|A\\|^2_F} ) \\mathbb{E}[  \\langle z_{k-1}-x_{\\star},\\sigma_{\\ell}(A) v_{\\ell}\\rangle ]\\notag\n",
      "    \\\\\n",
      "  &=(1 -\\frac{\\sigma_{\\ell}^2(A)}{\\|A\\|^2_F} ) \\mathbb{E}[  \\langle Az_{k-1}-Ax_{\\star},u_{\\ell}\\rangle ].  \\label{th81}\n",
      "\\end{align}\n",
      "\n",
      "We now consider $\\mathbb{E}[\\langle  A \\frac{(A^{(i)})^TA^{(i\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [29902, 2612, 1154, 8001, 29909]\n",
      "Target IDs (first 5): [2612, 1154, 8001, 29909, 7650]\n",
      "\n",
      "================ Sample 54 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      ")}}{\\| A^{(i)} \\|_{2}^{2}}( x_{k}-x_{\\star}),  u_{\\ell}   \\rangle]$. Exploiting (\\ref{th64}), we have\n",
      "\\begin{align}\n",
      " \\mathbb{E}[\\langle  A \\frac{(A^{(i)})^TA^{(i)}}{\\| A^{(i)} \\|_{2}^{2}}( x_{k}-x_{\\star}),  u_{\\ell}   \\rangle]\n",
      "&=\\mathbb{E}[\\langle    \\frac{(A^{(i)})^TA^{(i)}}{\\| A^{(i)} \\|_{2}^{2}}( x_{k}-x_{\\star}), A^T u_{\\ell}   \\rangle]\\notag\n",
      "\\\\\n",
      "&=\\sigma_\\ell (A)\\mathbb{E}[\\langle    \\frac{(A^{(i)})^TA^{(i)}}{\\| A^{(i)} \\|_{2}^{2}}( x_{k}-x_{\\star}), v_{\\ell}   \\rangle]\\notag\n",
      " \\\\\n",
      " &=\\frac{\\sigma_\\ell (A)}{\\|A\\|^2_F} (1-\\frac{\\sigma_\\ell^2(A)}{\\|A\\|^2_F})^k\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [29897, 12431, 29989, 319, 7650]\n",
      "Target IDs (first 5): [12431, 29989, 319, 7650, 29875]\n",
      "\n",
      "================ Sample 55 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "\\langle A^T(Ax_{0}- Ax_{\\star}),      v_{\\ell} \\rangle \\notag\n",
      "  \\\\\n",
      " &=\\frac{1}{\\|A\\|^2_F} (1-\\frac{\\sigma_\\ell^2(A)}{\\|A\\|^2_F})^k \\langle AA^T(Ax_{0}- Ax_{\\star}),      u_{\\ell} \\rangle. \\label{th82}\n",
      "\\end{align}\n",
      "\n",
      "Thus, combining (\\ref{th80}), (\\ref{th81}) and (\\ref{th82}) yields\n",
      "\\begin{align}\n",
      "&\\mathbb{E}[\\langle Az_{k}- Ax_{\\star}, u_{\\ell} \\rangle]  \\notag\n",
      " \\\\\n",
      "&= \\mathbb{E}[\\langle A(I-\\frac{(A^{(i)})^TA^{(i)}}{\\| A^{(i)} \\|_{2}^{2}})( z_{k-1}-x_{\\star}), u_{\\ell}  \\rangle] +\\mathbb{E}[\\langle A \\frac{(A^{(i)})^TA^{(i)}}{\\| A^{(i)} \\|_{2}^{2}}( x_{k}-x\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [320, 6990, 319, 29985, 29911]\n",
      "Target IDs (first 5): [6990, 319, 29985, 29911, 29898]\n",
      "\n",
      "================ Sample 56 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "_{\\star}),  u_{\\ell}  \\rangle] \\notag\n",
      " \\\\\n",
      "&=(1 -\\frac{\\sigma_{\\ell}^2(A)}{\\|A\\|^2_F} ) \\mathbb{E}[  \\langle Az_{k-1}-Ax_{\\star},u_{\\ell}\\rangle ]+\\frac{1}{\\|A\\|^2_F} (1-\\frac{\\sigma_\\ell^2(A)}{\\|A\\|^2_F})^k \\langle AA^T(Ax_{0}- Ax_{\\star}),      u_{\\ell} \\rangle \\notag\n",
      " \\\\\n",
      "&=(1 -\\frac{\\sigma_{\\ell}^2(A)}{\\|A\\|^2_F} )^2 \\mathbb{E}[  \\langle Az_{k-2}-Ax_{\\star},u_{\\ell}\\rangle ]+\\frac{2}{\\|A\\|^2_F} (1-\\frac{\\sigma_\\ell^2(A)}{\\|A\\|^2_F})^k \\langle AA^T(Ax_{0}- Ax_{\\star}),      u_{\\ell} \\rangle \\notag\n",
      " \\\\\n",
      "&=\\ldots=(1 -\\frac{\\sigma_{\\ell}^2(\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [1665, 8508, 9594, 29871, 318]\n",
      "Target IDs (first 5): [8508, 9594, 29871, 318, 1665]\n",
      "\n",
      "================ Sample 57 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "A)}{\\|A\\|^2_F} )^k   \\langle Az_{0}-Ax_{\\star}, v_{\\ell}\\rangle  +\\frac{k}{\\|A\\|^2_F} (1-\\frac{\\sigma_\\ell^2(A)}{\\|A\\|^2_F})^k \\langle AA^T(Ax_{0}- Ax_{\\star}),      u_{\\ell} \\rangle, \\notag\n",
      "\\end{align}\n",
      "which is the desired result (\\ref{th8}).\n",
      "\\end{pf}\n",
      "\n",
      "\\begin{rmk}\n",
      "\\label{rmk81}\n",
      "\n",
      "Theorem \\ref{theorem8} shows the decay rates of $\\|Az_{k}- Ax_{\\star}\\|$ of the REGS method and suggests that small singular values lead to poor convergence rates and vice versa. We note similar issues arise for the RK, REK, and RGS methods discussed in \\cite{steinerberger2021randomized}, \\cite{zhang2021preconvergence}, and Theorem \\ref{theorem1}, respectively.\n",
      "\n",
      "\\end{rmk}\n",
      "\n",
      "\n",
      "\\section{Numerical experiments }\\label{\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [29909, 19978, 29989, 29909, 7893]\n",
      "Target IDs (first 5): [19978, 29989, 29909, 7893, 29985]\n",
      "\n",
      "================ Sample 58 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "sec5}\n",
      "Now we present two simple examples to illustrate that the convergence directions of the RGS and REGS methods. To this end, let $G_0\\in R^{500\\times 500}$ be a Gaussian matrix with i.i.d. $N(0, 1)$ entries and $D\\in R^{500\\times 500}$ be a diagonal matrix whose diagonal elements are all 100. Further, we set $G_1=G_0+D$ and replace\n",
      "its last row $G_1^{(500)}$ by a tiny perturbation of $G_1^{(499)}$, i.e., adding 0.01 to each entry of $G_1^{(499)}$. Then, we normalize all rows of $G_1$, i.e., set $\\|G_1^{(i)}\\|_2=1$, $i=1, 2, \\ldots, 500$. After that, we set\n",
      "$A_1=\\begin{bmatrix}\n",
      "G_1\\\\\n",
      "G_2\n",
      "\\end{bmatrix}\n",
      "\\in R^{600\\\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [3471, 29945, 29913, 13, 10454]\n",
      "Target IDs (first 5): [29945, 29913, 13, 10454, 591]\n",
      "\n",
      "================ Sample 59 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "times 500}$ and\n",
      "$A_2=\\begin{bmatrix}\n",
      "G_1,\n",
      "G_3\n",
      "\\end{bmatrix}\n",
      "\\in R^{500\\times 600}$, where $G_2\\in R^{100\\times 500}$ and $G_3\\in R^{500\\times 100}$ are zero matrices. So, the first 499 singular values of the matrices $A_1$ and $A_2$ are between $\\sim 0.6$ and $\\sim 1.5$, and the smallest nonzero singular value is $\\sim 10^{-4}$.\n",
      "\n",
      "\n",
      "We first consider convergence directions of $Ax_k-Ax_{\\star}$ and $x_k-x_{\\star}$ of the RGS method. We generate a vector $x \\in R^{500}$ using the MATLAB function \\texttt{randn}, set the full column rank coefficient matrix $A=A_1$ and set the right-hand side $b=A x +z$, where $z$ is a nonzero vector belonging to the null space of $A^{T\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [3706, 29871, 29945, 29900, 29900]\n",
      "Target IDs (first 5): [29871, 29945, 29900, 29900, 1042]\n",
      "\n",
      "================ Sample 60 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "}$, which is generated by the MATLAB function \\texttt{null}. With $x_0=0$, we plot $|\\langle (Ax_k-Ax_{\\star})/\\|Ax_k-Ax_{\\star}\\|_2, u_{500} \\rangle|$ and $\\frac{\\|A(x_k-x_{\\star})\\|_2 }{\\|x_k-x_{\\star}\\|_2}$ in Figure \\ref{fig1} and Figure \\ref{fig2}, respectively.\n",
      "\n",
      "\\begin{figure}[ht]\n",
      " \\begin{center}\n",
      "\\includegraphics [height=5.5cm,width=8.5cm  ]{RGS-Ax-u-600-500.eps}\n",
      " \\end{center}\n",
      "\\caption{A sample evolution of $ |\\langle (Ax_k-Ax_{\\star})/\\|Ax_k-Ax_{\\star}\\|_2, u_{500} \\rangle|$ of the RGS method. }\\label{fig1}\n",
      "\\end{figure}\n",
      "From Figure \\ref{fig1}, we find that $|\\langle (Ax_k\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [4429, 607, 338, 5759, 491]\n",
      "Target IDs (first 5): [607, 338, 5759, 491, 278]\n",
      "\n",
      "================ Sample 61 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "-Ax_{\\star})/\\|Ax_k-Ax_{\\star}\\|_2, u_{500} \\rangle|$ initially is very small and almost is 0, which indicates that $Ax_k-Ax_{\\star} $ is not close to the left singular vector $u_{500}$. Considering the analysis of Remark \\ref{rmk5}, the phenomenon implies the `preconvergence' behavior of the RGS method, that is, the RGS method seems to converge quickly at the beginning. In addition, as $k\\rightarrow\\infty$, $|\\langle (Ax_k-Ax_{\\star})/\\|Ax_k-Ax_{\\star}\\|_2, u_{500} \\rangle|\\rightarrow 1$. This phenomenon implies that $Ax_{k}-Ax_{\\star}$ tends to the left singular vector corresponding to the smallest singular value of $A$.\n",
      "\n",
      "\\begin{figure}[ht]\n",
      " \\begin{center}\n",
      "\\includegraphics [height=5.5cm,width=8.5cm  ]{RGS-x-600-500.eps}\n",
      " \\end\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [29899, 29909, 29916, 1665, 8508]\n",
      "Target IDs (first 5): [29909, 29916, 1665, 8508, 1800]\n",
      "\n",
      "================ Sample 62 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "{center}\n",
      "\\caption{A sample evolution of $\\frac{\\|A(x_k-x_{\\star})\\|_2 }{\\|x_k-x_{\\star}\\|_2}$ of the RGS method. }\\label{fig2}\n",
      "\\end{figure}\n",
      "\n",
      "From Figure \\ref{fig2}, we observe that the values of $\\frac{\\|A(x_k-x_{\\star})\\|_2 }{\\|x_k-x_{\\star}\\|_2}$ decreases with $k$ and finally approaches the small singular value. This phenomenon implies that the forward direction of $x_k-x_{\\star}$ is mainly determined by the right singular vectors corresponding to the large singular values of $A$ at the beginning. With the increase of $k$, the direction is mainly determined by the right singular vectors corresponding to the small singular values. Finally, $x_k-x_{\\star}$ tends to the right singular vector space corresponding to the smallest singular value. Furthermore, this phenomenon also allows for an interesting application, i.e., finding nonzero vectors $x$ such that $\\frac{\\|Ax\\|_2}{\\|x\\|_2}$ is small.\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [29912, 5064, 29913, 13, 29905]\n",
      "Target IDs (first 5): [5064, 29913, 13, 29905, 6671]\n",
      "\n",
      "================ Sample 63 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "\n",
      "\n",
      "We now consider convergence directions of $Az_k-Ax_{\\star}$ and $z_k-x_{\\star}$ of the REGS method. We generate a vector $x \\in R^{600}$ using the MATLAB function \\texttt{randn}, set the coefficient matrix $A=A_2$ which does not have full column rank, and set the right-hand side $b=Ax  $. With $x_0=0$ and $z_0=0$, we plot $|\\langle (Az_k-Ax_{\\star})/\\|Az_k-Ax_{\\star}\\|_2, u_{500} \\rangle|$ and $\\frac{\\|A(z_k-x_{\\star})\\|_2 }{\\|z_k-x_{\\star}\\|_2}$ in Figure \\ref{fig3} and Figure \\ref{fig4}, respectively.\n",
      "\\begin{figure}[ht]\n",
      " \\begin{center}\n",
      "\\includegraphics [height=5.5cm,width=8.5cm  ]{REGS-Az-u-500-600.eps}\n",
      " \\end{center}\n",
      "\\\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [13, 13, 4806, 1286, 2050]\n",
      "Target IDs (first 5): [13, 4806, 1286, 2050, 17221]\n",
      "\n",
      "================ Sample 64 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "caption{A sample evolution of $ |\\langle (Az_k-Ax_{\\star})/\\|Az_k-Ax_{\\star}\\|_2, u_{500} \\rangle|$ of the REGS method. }\\label{fig3}\n",
      "\\end{figure}\n",
      "\n",
      "\\begin{figure}[ht]\n",
      " \\begin{center}\n",
      "\\includegraphics [height=5.5cm,width=8.5cm  ]{REGS-z-500-600.eps}\n",
      " \\end{center}\n",
      "\\caption{A sample evolution of $\\frac{\\|A(z_k-x_{\\star})\\|_2 }{\\|z_k-x_{\\star}\\|_2}$ of the REGS method. }\\label{fig4}\n",
      "\\end{figure}\n",
      "\n",
      "Figure \\ref{fig3} and Figure \\ref{fig4} show the similar results obtained in the RGS method. That is, the convergence directions of $A z_k-Ax_{\\star} $ and $ z_k- x_{\\star} $ of the REGS method initially are depending on the large singular values and then mainly depending on the small singular values,\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [6671, 29912, 29909, 4559, 14675]\n",
      "Target IDs (first 5): [29912, 29909, 4559, 14675, 310]\n",
      "\n",
      "================ Sample 65 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "and finally depending on the smallest singular value of $A$.</s><s> A note on semi-infinite program bounding methods\n",
      "\n",
      "Semi-infinite programs are a class of mathematical optimization problems with a finite number of decision variables and infinite constraints. As shown by Blankenship and Falk (Blankenship and Falk. \"Infinitely constrained optimization problems.\" Journal of Optimization Theory and Applications 19.2 (1976): 261-281.), a sequence of lower bounds which converges to the optimal objective value may be obtained with specially constructed finite approximations of the constraint set. In (Mitsos. \"Global optimization of semi-infinite programs via restriction of the right-hand side.\" Optimization 60.10-11 (2011): 1291-1308.), it is claimed that a modification of this lower bounding method involving approximate solution of the lower-level program yields convergent lower bounds. We show with a counterexample that this claim is false, and discuss what kind of approximate solution of the lower-level program is sufficient for correct behavior.\n",
      "\n",
      "\\section{Introduction}\n",
      "This note discuss\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [322, 7146, 8679, 373, 278]\n",
      "Target IDs (first 5): [7146, 8679, 373, 278, 19087]\n",
      "\n",
      "================ Sample 66 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "es methods for the global solution of semi-infinite programs (SIP).\n",
      "Specifically, the method from \\cite{mitsos11} is considered, and it is shown with a counterexample that the lower bounds do not always converge.\n",
      "Throughout we use notation as close as possible to that used in \\cite{mitsos11}, embellishing it only as necessary with, for instance, iteration counters. \n",
      "\n",
      "Consider a SIP in the general form\n",
      "\\begin{alignat}{2} \n",
      "\\tag{SIP}\n",
      "\\label{eq:sip}\n",
      "f^* = \n",
      "\\inf_{x}\\; & f(x) \\\\\n",
      "\\st \t\t\t\t\n",
      "\\notag & x \\in X, \\\\\n",
      "\\notag & g(x,y) \\le 0,\\; \\forall y \\in Y,\n",
      "\\end{alignat}\n",
      "for subsets $X$, $Y$ of finite dimensional real vector spaces and \n",
      "$f : X \\to \\mbb{R}$,\n",
      "$g : X \\times Y \\to \\mbb{R}$.\n",
      "We may view $Y$ as an\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [267, 3519, 363, 278, 5534]\n",
      "Target IDs (first 5): [3519, 363, 278, 5534, 1650]\n",
      "\n",
      "================ Sample 67 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "index set, with potentially uncountably infinite cardinality.\n",
      "Important to validating the feasibility of a point $x$ is the lower-level program:\n",
      "\\begin{equation}\n",
      "\\label{eq:llp}\n",
      "\\tag{LLP}\n",
      "\\sup_y \\set{ g(x,y) : y \\in Y}.\n",
      "\\end{equation}\n",
      "\n",
      "Global solution of \\eqref{eq:sip} often involves the construction of convergent upper and lower bounds.\n",
      "The approach in \\cite{mitsos11} to obtain a lower bound is a modification of the constraint-generation/discretization method of \\cite{blankenshipEA76}.\n",
      "The claim is that the lower-level program may be solved approximately;\n",
      "the exact nature of the approximation is important to the convergence of the lower bounds and this is the subject of the present note.\n",
      "\n",
      "\n",
      "\n",
      "\\section{Sketch of the lower bounding procedure and claim}\n",
      "\n",
      "The setting of the method is the following.\n",
      "The method is iterative and at iteration $k$, for a given finite subset $Y^{LBD,k} \\subset Y$, a lower bound of $f\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [2380, 731, 29892, 411, 19998]\n",
      "Target IDs (first 5): [731, 29892, 411, 19998, 443]\n",
      "\n",
      "================ Sample 68 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "^*$ is obtained from the finite program\n",
      "\\begin{alignat}{2} \n",
      "\\label{eq:sip_lower}\n",
      "f^{LBD,k} = \n",
      "\\inf_{x}\\; & f(x) \\\\\n",
      "\\st \t\t\t\t\n",
      "\\notag & x \\in X, \\\\\n",
      "\\notag & g(x,y) \\le 0, \\;\\forall y \\in Y^{LBD,k}.\n",
      "\\end{alignat}\n",
      "This is indeed a lower bound since fewer constraints are enforced, and thus \\eqref{eq:sip_lower} is a relaxation of \\eqref{eq:sip}.\n",
      "Assume that the lower bounding problem \\eqref{eq:sip_lower} is feasible\n",
      "(otherwise we can conclude that \\eqref{eq:sip} is infeasible).\n",
      "Let $\\bar{x}^k$ be a (global) minimizer of the lower bounding problem \\eqref{eq:sip_lower}.\n",
      "In \\cite{mitsos11}, Lemma~2.2 states that we either verify \n",
      "$\\sup_y \\set{g(\\bar{\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [6622, 29938, 338, 7625, 515]\n",
      "Target IDs (first 5): [29938, 338, 7625, 515, 278]\n",
      "\n",
      "================ Sample 69 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "x}^k,y) : y \\in Y} \\le 0$,\n",
      "\\textbf{or else} find $\\bar{y}^k \\in Y$ such that $g(\\bar{x}^k,\\bar{y}^k) > 0$.\n",
      "If $\\sup_y \\set{g(\\bar{x}^k,y) : y \\in Y} \\le 0$,\n",
      "then $\\bar{x}^k$ is feasible in \\eqref{eq:sip} and thus optimal (since it also solves a relaxation).\n",
      "Otherwise, set $Y^{LBD,k+1}  = Y^{LBD,k} \\cup \\set{\\bar{y}^k}$ and we iterate.\n",
      "\n",
      "The precise statement of the claim is repeated here (again, with only minor embellishments to the notation to help keep track of iterations).\n",
      "\n",
      "\\begin{lemma}[Lemma~2.2 in \\cite{mitsos11}]\n",
      "\\label{lem:claim}\n",
      "Take any $Y^{LBD,0} \\subset Y$.\n",
      "Assume that $X$ and $Y$ are compact and that $g$\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [29916, 2137, 29895, 29892, 29891]\n",
      "Target IDs (first 5): [2137, 29895, 29892, 29891, 29897]\n",
      "\n",
      "================ Sample 70 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "is continuous on $X \\times Y$.\n",
      "Suppose that at each iteration of the lower bounding procedure the lower-level program is solved approximately for the solution of the lower bounding problem $\\bar{x}^k$ either establishing \n",
      "$\\max_{y \\in Y} g(\\bar{x}^k,y) \\le 0$, or furnishing a point $\\bar{y}^k$ such that $g(\\bar{x}^k,\\bar{y}^k) > 0$.\n",
      "Then, the lower bounding procedure converges to the optimal objective value, i.e. $f^{LBD,k} \\to f^*$.\n",
      "\\end{lemma}\n",
      "\n",
      "\\section{Correction}\n",
      "\n",
      "\\subsection{Counterexample}\n",
      "\n",
      "We now present a counterexample to the claim in Lemma~\\ref{lem:claim}.\n",
      "Consider \n",
      "\\begin{alignat}{2} \n",
      "\\tag{CEx}\n",
      "\\label{eq:counter_ex}\n",
      "\\inf_{x}\\; & -x \\\\\n",
      "\\st \t\t\t\t\n",
      "\\notag & x \\in [-1,1], \\\\\n",
      "\\notag & 2x -\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [338, 9126, 373, 395, 29990]\n",
      "Target IDs (first 5): [9126, 373, 395, 29990, 320]\n",
      "\n",
      "================ Sample 71 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "y \\le 0,\\; \\forall y \\in [-1,1],\n",
      "\\end{alignat}\n",
      "thus we define \n",
      "$X = Y = [-1,1]$, \n",
      "$f : x \\mapsto -x$,\n",
      "$g : (x,y) \\mapsto 2x - y$.\n",
      "The behavior to note is this:\n",
      "We are trying to maximize $x$;\n",
      "The feasible set is\n",
      "\\[\n",
      "\\set{x \\in [-1,1] : x \\le (\\sfrac{1}{2})y, \\forall y \\in[-1,1] } = [-1,-\\sfrac{1}{2}];\n",
      "\\]\n",
      "The infimum, consequently, is $\\sfrac{1}{2}$.\n",
      "See Figure~\\ref{fig:cex1}.\n",
      "\n",
      "\\begin{figure}\n",
      "\\begin{center}\n",
      "\\begin{tikzpicture}[xscale=1.5,yscale=1.5]\n",
      "\\draw[fill=gray] (-0.5,-1) -- (0.5,1) -- (1,1) -- (1,-1) -- cycle;\n",
      "\\draw[latex-latex\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [343, 320, 280, 29871, 29900]\n",
      "Target IDs (first 5): [320, 280, 29871, 29900, 2053]\n",
      "\n",
      "================ Sample 72 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "] (0,-1.3) -- (0,1.3) node[above]{$y$};\n",
      "\\draw[latex-latex] (-1.3,0) -- (1.3,0) node[right]{$x$};\n",
      "\\draw (-1,-1) rectangle (1,1);\n",
      "\\draw[dashed,domain=0:1] plot (\\x, \\x);\n",
      "\\end{tikzpicture}\n",
      "\\end{center}\n",
      "\\caption{Visualization of counterexample~\\eqref{eq:counter_ex}.\n",
      "The box represents $[-1,1] \\times [-1,1]$.\n",
      "The shaded grey area is the subset of $(x,y)$ such that $2x - y > 0$.\n",
      "The dashed line represents the approximate minimizers used in the counterexample.}\n",
      "\\label{fig:cex1}\n",
      "\\end{figure}\n",
      "\n",
      "\n",
      "Beginning with $Y^{LBD,1} = \\emptyset$,  the minimizer of the lower bounding problem is $\\bar{x}^1 = 1$.\n",
      "Now, assume that solving the resulting \\eqref{eq:llp} approximately, we get $\\bar{y}^1\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [29962, 313, 29900, 6653, 29896]\n",
      "Target IDs (first 5): [313, 29900, 6653, 29896, 29889]\n",
      "\n",
      "================ Sample 73 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "= 1$ which we note satisfies\n",
      "\\[\n",
      "2\\bar{x}^1 - \\bar{y}^1 = 1 > 0\n",
      "\\]\n",
      "as required by Lemma~\\ref{lem:claim}.\n",
      "\n",
      "The next iteration, with $Y^{LBD,2} = \\set{1}$, adds the constraint \n",
      "$2x - 1 \\le 0$\n",
      "to the lower bounding problem;\n",
      "the feasible set is $[-1,\\sfrac{1}{2}]$ so the minimizer is $\\bar{x}^2 = \\sfrac{1}{2}$.\n",
      "Again, assume that solving the lower-level program approximately yields $\\bar{y}^2 = \\sfrac{1}{2}$;\n",
      "again we get\n",
      "\\[\n",
      "2\\bar{x}^2 - \\bar{y}^2 = \\sfrac{1}{2} > 0\n",
      "\\]\n",
      "as required by Lemma~\\ref{lem:claim}.\n",
      "\n",
      "The third iteration, with $Y^{LBD,3} = \\set{1, \\sfrac{1}{2}}$, adds the constraint \n",
      "$2x - \\sfrac{1}{2}\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [353, 29871, 29896, 29938, 607]\n",
      "Target IDs (first 5): [29871, 29896, 29938, 607, 591]\n",
      "\n",
      "================ Sample 74 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "\\le 0$\n",
      "to the lower bounding problem;\n",
      "the feasible set is $[-1,\\sfrac{1}{4}]$ so the minimizer is $\\bar{x}^3 = \\sfrac{1}{4}$.\n",
      "Again, assume that solving the lower-level program approximately yields $\\bar{y}^3 = \\sfrac{1}{4}$;\n",
      "again we get\n",
      "\\[\n",
      "2\\bar{x}^3 - \\bar{y}^3 = \\sfrac{1}{4} > 0\n",
      "\\]\n",
      "as required by Lemma~\\ref{lem:claim}.\n",
      "\n",
      "Proceeding in this way, we construct $\\bar{x}^k$ and $\\bar{y}^k$ so that \n",
      "$g(\\bar{x}^k,\\bar{y}^k) > 0$ and \n",
      "the lower bounds satisfy \n",
      "$f^{LBD,k} = -\\bar{x}^k = -\\frac{1}{2^{k-1}}$, for all $k$.\n",
      "Consequently, they converge to $0$, which we note is strictly less than the infimum of $\\sfrac{1}{2}$.\n",
      "\n",
      "\n",
      "\\subsection\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [320, 280, 29871, 29900, 29938]\n",
      "Target IDs (first 5): [280, 29871, 29900, 29938, 13]\n",
      "\n",
      "================ Sample 75 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "{Modified claim}\n",
      "\n",
      "We now present a modification of the claim in order to demonstrate what kind of approximate solution of the lower-level program suffices to establish convergence of the lower bounds.\n",
      "To state the result, let the optimal objective value of \\eqref{eq:llp} as a function of $x$ be\n",
      "\\[\n",
      "g^*(x) = \\sup_y\\set{g(x,y) : y \\in Y}.\n",
      "\\]\n",
      "The proof of the following result has a similar structure to the original proof of \\cite[Lemma~2.2]{mitsos11}.\n",
      "\n",
      "\\begin{lemma}\n",
      "\\label{lem:claim_mod}\n",
      "Choose any finite $Y^{LBD,0} \\subset Y$, and $\\alpha \\in (0,1)$.\n",
      "Assume that $X$ and $Y$ are compact and that $f$ and $g$ are continuous.\n",
      "Suppose that at each iteration $k$ of the lower bounding procedure \\eqref{eq:llp} is solved approximately for the solution $\\bar{x}^k$ of the lower bounding problem~\\eqref{eq:sip_lower}, either\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [29912, 2111, 2164, 5995, 29913]\n",
      "Target IDs (first 5): [2111, 2164, 5995, 29913, 13]\n",
      "\n",
      "================ Sample 76 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "establishing that\n",
      "$g^*(\\bar{x}^k) \\le 0$ \n",
      "or\n",
      "furnishing a point $\\bar{y}^k$ such that \n",
      "\\[\n",
      "g(\\bar{x}^k,\\bar{y}^k) \\ge \\alpha g^*(\\bar{x}^k) > 0.\n",
      "\\]\n",
      "Then, the lower bounding procedure converges to the optimal objective value, i.e. $f^{LBD,k} \\to f^*$. \n",
      "\\end{lemma}\n",
      "\\begin{proof}\n",
      "First, if the lower bounding problem~\\eqref{eq:sip_lower} is ever infeasible for some iteration $k$, then \\eqref{eq:sip} is infeasible and we can set $f^{LBD,k} = +\\infty = f^*$.\n",
      "Otherwise, since $X$ is compact, $Y^{LBD,k}$ is finite, and $f$ and $g$ are continuous, for every iteration the lower bounding problem has a solution by Weierstrass' (extreme value) theorem.\n",
      "If at some iteration $k$ the lower bounding problem furnishes\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [10127, 292, 393, 13, 29938]\n",
      "Target IDs (first 5): [292, 393, 13, 29938, 29887]\n",
      "\n",
      "================ Sample 77 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "a point $\\bar{x}^k$ for which $g^*(\\bar{x}^k) \\le 0$, then $\\bar{x}^k$ is feasible for \\eqref{eq:sip}, and thus optimal.\n",
      "The corresponding lower bound $f^{LBD,k}$, and all subsequent lower bounds, equal $f^*$.\n",
      "\n",
      "Otherwise, we have an infinite sequence of solutions to the lower bounding problems.\n",
      "Since $X$ is compact we can move to a subsequence $\\seq[k \\in \\mbb{N}]{\\bar{x}^{k}} \\subset X$ which converges to $x^* \\in X$.\n",
      "By construction of the lower bounding problem we have\n",
      "\\[\n",
      "g(\\bar{x}^{\\ell},\\bar{y}^k) \\le 0, \\quad \\forall \\ell,k : \\ell > k.\n",
      "\\]\n",
      "By continuity and compactness of $X \\times Y$ we have uniform continuity of $g$, and so for any $\\epsilon > 0$, there exists a $\\delta > 0$ such that\n",
      "\\begin{equation}\n",
      "\\label{eq:gee}\n",
      "g(x,\\bar{\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [263, 1298, 779, 1646, 29912]\n",
      "Target IDs (first 5): [1298, 779, 1646, 29912, 29916]\n",
      "\n",
      "================ Sample 78 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "y}^k) < \\epsilon,\n",
      "\t\\quad\\forall x : \\norm{x - \\bar{x}^{\\ell}} < \\delta, \n",
      "\t\t\\quad\\forall \\ell,k : \\ell > k.\n",
      "\\end{equation}\n",
      "Since the (sub)sequence $\\seq[k \\in \\mbb{N}]{\\bar{x}^k}$ converges, there is an index $K$ sufficiently large that \n",
      "\\begin{equation}\n",
      "\\label{eq:tails}\n",
      "\\norm{\\bar{x}^{\\ell} - \\bar{x}^k} < \\delta, \\quad \\forall \\ell,k : \\ell > k \\ge K.\n",
      "\\end{equation}\n",
      "Using \\eqref{eq:tails}, we can substitute $x = \\bar{x}^k$ in \\eqref{eq:gee} to get that for any $\\epsilon > 0$, there exists $K$ such that\n",
      "\\[\n",
      "g(\\bar{x}^k,\\bar{y}^k) < \\epsilon, \\quad \\forall k \\ge K.\n",
      "\\]\n",
      "By assumption $g(\\bar{x}^k,\\bar{y}^k)\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [29891, 2137, 29895, 29897, 529]\n",
      "Target IDs (first 5): [2137, 29895, 29897, 529, 320]\n",
      "\n",
      "================ Sample 79 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "> 0$ for all $k$, and so combined with the above we have that\n",
      "$g(\\bar{x}^k,\\bar{y}^k) \\to 0$. \n",
      "\n",
      "Combining $g(\\bar{x}^k,\\bar{y}^k) \\to 0$ with\n",
      "$\n",
      "g(\\bar{x}^k,\\bar{y}^k) \\ge \\alpha g^*(\\bar{x}^k) > 0,\n",
      "$\n",
      "for all $k$,\n",
      "we see \n",
      "$\n",
      "g^*(\\bar{x}^k) \\to 0.\n",
      "$\n",
      "Meanwhile $g^* : X \\to \\mbb{R}$ is a continuous function, by classic parametric optimization results like \\cite[Theorem~1.4.16]{aubin_frankowska} (using continuity of $g$ and compactness of $Y$).\n",
      "Thus \n",
      "\\[\n",
      "g^*(x^*)  = \\lim_{k \\to \\infty} g^*(\\bar{x}^k) = 0.\n",
      "\\]\n",
      "Thus $x^*$ is feasible in \\eqref{eq:sip} and so\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [1405, 29871, 29900, 29938, 363]\n",
      "Target IDs (first 5): [29871, 29900, 29938, 363, 599]\n",
      "\n",
      "================ Sample 80 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "$f^* \\le f(x^*)$.\n",
      "But since the lower bounding problem is a relaxation, $f^{LBD,k} = f(\\bar{x}^k) \\le f^*$ for all $k$, and so by continuity of $f$, $f(x^*) \\le f^*$.\n",
      "Combining these inequalities we see $f^{LBD,k} \\to f(x^*) = f^*$.\n",
      "Since the entire sequence of lower bounds is an increasing sequence, we see that the entire sequence converges to $f^*$ (without moving to a subsequence).\n",
      "\\end{proof}\n",
      "\\section{Remarks}\n",
      "\n",
      "The main contribution of \\cite{mitsos11} is a novel \\emph{upper} bounding procedure, which still stands, and combined with the modified lower bounding procedure from Lemma~\\ref{lem:claim_mod} or the original procedure from \\cite{blankenshipEA76}, the overall global solution method for \\eqref{eq:sip} is still effective.\n",
      "\n",
      "The counterexample that has been presented may seem contrived.\n",
      "However, as the lower bounding method for\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [395, 29888, 6622, 320, 280]\n",
      "Target IDs (first 5): [29888, 6622, 320, 280, 285]\n",
      "\n",
      "================ Sample 81 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "SIP from \\cite{mitsos11} is adapted to give a lower bounding method for \\emph{generalized} semi-infinite programs (GSIP) in \\cite{mitsosEA15}, a modification of the counterexample reveals that similar behavior may occur (and in a more natural way) when constructing the lower bounds for a GSIP.\n",
      "Consequently, the lower bounds fail to converge to the infimum.\n",
      "See \\cite{Harwood19_GSIP}.</s><s> A fixed point theorem for the infinite-dimensional simplex\n",
      "\n",
      "We define the infinite dimensional simplex to be the closure of the convex hull of the standard basis vectors in R^infinity, and prove that this space has the 'fixed point property': any continuous function from the space into itself has a fixed point. Our proof is constructive, in the sense that it can be used to find an approximate fixed point; the proof relies on elementary analysis and Sperner's lemma. The fixed point theorem is shown to imply Schauder's fixed point theorem on infinite-dimensional compact convex subsets of normed spaces.\n",
      "\n",
      "\\section{Introduction}\n",
      "\n",
      "In finite\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [317, 5690, 515, 320, 2036]\n",
      "Target IDs (first 5): [5690, 515, 320, 2036, 29912]\n",
      "\n",
      "================ Sample 82 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "dimensions, one of the simplest methods for proving the\n",
      "Brouwer fixed point theorem is via a combinatorial result known as\n",
      "Sperner's lemma \\cite{Sper28}, which is a statement about labelled\n",
      "triangulations of a simplex in $\\ensuremath{\\mathbb{R}} ^n$.  In this paper, we use\n",
      "Sperner's lemma to prove a fixed point theorem on an \n",
      "infinite-dimensional simplex in $\\ensuremath{\\mathbb{R}} ^\\infty$.  We also show that this\n",
      "theorem implies the infinite-dimensional case of Schauder's fixed point\n",
      "theorem on normed spaces.\n",
      "\n",
      "Since $\\ensuremath{\\mathbb{R}} ^\\infty$ is locally convex, our theorem \n",
      "is a consequence of Tychonoff's fixed point theorem \\cite{Smar74}.\n",
      "However, some notable advantages of our approach are: \n",
      "(1) the constructive nature of Sperner's lemma provides a method for\n",
      "producing approximate fixed points for functions on the\n",
      "infinite-dimensional simplex, (2) the proof is based on elementary\n",
      "methods in topology and analysis, and (3) our proof provides another\n",
      "route\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [13391, 29892, 697, 310, 278]\n",
      "Target IDs (first 5): [29892, 697, 310, 278, 20393]\n",
      "\n",
      "================ Sample 83 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "to Schauder's theorem.\n",
      "\n",
      "Fixed point theorems and their constructive proofs \n",
      "have found many important applications, ranging from proofs \n",
      "of the Inverse Function Theorem \\cite{Lang97}, to proofs of the\n",
      "existence of equilibria in economics \\cite{Todd76, Yang99}, to the \n",
      "existence of solutions of differential equations \\cite{Brow93, Smar74}.\n",
      "\n",
      "\n",
      "\\section{Working in $\\ensuremath{\\mathbb{R}} ^\\infty$}\n",
      "Let $\\ensuremath{\\mathbb{R}} ^\\infty$ and $I^\\infty = \\prod [0,1]$ be the product of \n",
      "countably many copies of $\\ensuremath{\\mathbb{R}} $, and $I=[0,1]$, respectively.  \n",
      "We equip $\\ensuremath{\\mathbb{R}} ^\\infty$ with the standard product topology, which is \n",
      "metrizable \\cite{BePe75} by the complete metric\n",
      "\\[\n",
      "\\bar d(x,y) = \\sum_{i=1}^\\infty \\frac{|x_i - y_i|}{2^i(1+|x_i-y\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [304, 1102, 15052, 261, 29915]\n",
      "Target IDs (first 5): [1102, 15052, 261, 29915, 29879]\n",
      "\n",
      "================ Sample 84 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "_i|)}.\n",
      "\\]\n",
      "In $\\ensuremath{\\mathbb{R}} ^n$, a $k$-dimensional simplex, or \\textit{$k$-simplex}, $\\sigma^k$\n",
      "is the convex hull of $k+1$ affinely independent points.\n",
      "The \\textit{standard $n$-simplex} in $\\ensuremath{\\mathbb{R}} ^{n+1}$, denoted $\\Delta^n$,\n",
      "is the convex hull of the $n+1$ standard basis vectors of $\\ensuremath{\\mathbb{R}} ^n$. \n",
      "\n",
      "The natural extension of this definition to $\\ensuremath{\\mathbb{R}} ^\\infty$ is \n",
      "to consider $\\Delta^\\infty$, the convex hull of the standard basis\n",
      "vectors $\\{e_i\\}$ in $\\ensuremath{\\mathbb{R}} ^\\infty$, where $(e_i)_j= \\delta_{ij}$, the\n",
      "Kronecker delta function.  As convex combinations are finite\n",
      "sums, this convex hull is:\n",
      "\\[\n",
      "\\ensuremath{\\Delta^\\infty}  = \\{x\\in \\ensuremath{\\mathbb{R}} ^\\infty | \\sum_{i=1}^\\infty x_i = 1,\\\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [29918, 29875, 29989, 29512, 13]\n",
      "Target IDs (first 5): [29875, 29989, 29512, 13, 18899]\n",
      "\n",
      "================ Sample 85 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      " 0\\leq x_i \\leq\n",
      "1 \\mbox{, and only finitely many $x_i$ are non-zero}\\}.\n",
      "\\]\n",
      "Unfortunately, $\\ensuremath{\\Delta^\\infty} $ is not closed; under \n",
      "the metric $\\bar d$ the sequence $\\{e_i\\}$ converges to $\\mathbf{0}$, \n",
      "which is not in $\\ensuremath{\\Delta^\\infty} $.\n",
      "So consider, instead $\\ensuremath{\\Delta^\\infty_0} $, the closure of $\\ensuremath{\\Delta^\\infty} $, which can be shown to be:\n",
      "\\[  \n",
      "\\ensuremath{\\Delta^\\infty_0}   = \\{x\\in \\ensuremath{\\mathbb{R}} ^\\infty | \\sum_{i=1}^\\infty x_i \\leq 1 \\mbox{ and }\n",
      "0\\leq x_i\\leq 1\\}.\n",
      "\\]\n",
      "It is easy to see that $\\ensuremath{\\Delta^\\infty_0} $ is convex.  It is also the closure\n",
      "of the convex hull of \n",
      "the standard basis vectors $\\{e_i\\}$ and $\\mathbf{0}$.\n",
      "   It is also compact because it is a closed subset of $I^\\infty$, \n",
      "  \n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [259, 29900, 29905, 3797, 921]\n",
      "Target IDs (first 5): [29900, 29905, 3797, 921, 29918]\n",
      "\n",
      "================ Sample 86 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "which is compact by Tychonoff's Theorem.\n",
      "      \\footnote{If one would like to avoid the Axiom\n",
      "\tof Choice, which is equivalent to Tychonoff's Theorem, it is not\n",
      "\tdifficult to show that $I^\\infty$ is a closed and totally bounded\n",
      "\tsubset of the complete space $\\ensuremath{\\mathbb{R}} ^\\infty$, which implies\n",
      "\tcompactness.}  \n",
      "We call $\\ensuremath{\\Delta^\\infty_0} $ the {\\em standard infinite-dimensional simplex}.\n",
      "\n",
      "It will be important for our purposes later to consider $F^n$, \n",
      "   the $n$-dimensional face of $\\ensuremath{\\Delta^\\infty_0} $ given by $F^n = conv\\{e_1,e_2,\\dots,\n",
      "\te_{n+1}\\}$. Notice that each $F^n$ is closed and thus compact.\n",
      "\n",
      "\n",
      "\\section{Some Preliminary Machinery}\n",
      "\n",
      "\\begin{comment\n",
      "Before stating Sperner's Lemma, which will be indispensable in our\n",
      "proof, some definitions are necessary.   Let $\\sigma^k =\n",
      "conv(x_0,...,\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [607, 338, 11071, 491, 323]\n",
      "Target IDs (first 5): [338, 11071, 491, 323, 3376]\n",
      "\n",
      "================ Sample 87 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "x_k)$ be a $k$-simplex in $\\ensuremath{\\mathbb{R}} ^n$.   Further, let $A$ be the\n",
      "simplexes of a triangulation of $\\sigma^k$ and $B$ be the set of\n",
      "vertices of $A$.   An A-Sperner map for this triangulation of\n",
      "$\\sigma^k$ is a map $h:B\\rightarrow \\{0,\\dots,k\\}$ such that, if\n",
      "\\[ J \\subseteq \\{0,...,k\\} \\mbox{ and } v \\in conv\\{x_j | j \\in J\\}\n",
      "\\mbox{ then } h(v) \\in J.\\]\n",
      "A simplex in the triangulation of $\\sigma^k$ is called full if the\n",
      "image of its vertices under $h$ maps onto\n",
      "$\\{0,\\dots,k\\}$. (Definitions from van Mill p. 103)\n",
      "\n",
      "\\begin{sperner} (van Mill p.103) Let $\\sigma^k$ be a $k$-simplex in $\\ensuremath{\\mathbb{R}} ^n$ and let $A$ be the set of simplexes in a triang\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [29916, 29918, 29895, 1262, 367]\n",
      "Target IDs (first 5): [29918, 29895, 1262, 367, 263]\n",
      "\n",
      "================ Sample 88 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "ulation of $\\sigma^k$.   If $h$ is an A-Sperner map for $\\sigma^k$  then the number of full simplexes in $A$ is odd and hence non-zero. \\end{sperner}\n",
      "\n",
      "\\end{comment\n",
      "\n",
      "Let $\\sigma^k = conv(x_0,...,x_k)$ be a $k$-simplex in $\\ensuremath{\\mathbb{R}} ^n$.   \n",
      "Let $T$ be a triangulation of $\\sigma^k$ and $V$ be the set of\n",
      "vertices of $T$ (i.e., the vertices of simplices in $T$).   \n",
      "A \\emph{Sperner labelling} of the triangulation $T$ \n",
      "is a labelling function $\\ell:V\\rightarrow \\{0,\\dots,k\\}$ such that \n",
      "\\[ \n",
      "\\mbox{ if } J \\subseteq \\{0,\\dots,k\\} \\mbox{ and } v \\in conv\\{x_j | j \\in J \\}\n",
      "\\mbox{, then } h(v) \\in J.\n",
      "\\]\n",
      "A $k$-simplex $\\tau$ of $T$ \n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [2785, 310, 779, 3754, 29985]\n",
      "Target IDs (first 5): [310, 779, 3754, 29985, 29895]\n",
      "\n",
      "================ Sample 89 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "\n",
      "is called a {\\em fully-labelled simplex} (or {\\em full})\n",
      "if the image of the vertices of $\\tau$ under $\\ell$ maps onto\n",
      "$\\{0,\\dots,k\\}$.  Note that $\\tau$ has exactly $k+1$ vertices, so all\n",
      "the vertices have distinct labels.\n",
      "\n",
      "\\begin{sperner} \n",
      "Let $\\sigma^k$ be a $k$-simplex in $\\ensuremath{\\mathbb{R}} ^n$ with triangulation $T$ and let\n",
      "$\\ell$ be a Sperner-labelling of $T$.\n",
      "Then the number of full simplices of $T$ is odd (and hence, non-zero). \n",
      "\\end{sperner}\n",
      "\n",
      "Though we will not prove this theorem here, an exposition of such \n",
      "proofs can be found in\n",
      "\\cite{Su99}.  In particular, there are constructive\n",
      "``path-following'' proofs that locate the full simplex by tracing a\n",
      "path of simplices through the triangulation.  Such path-following\n",
      "proofs have formed the basis of\n",
      "algorithms for locating fixed points of functions in \n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [13, 275, 2000, 263, 2802]\n",
      "Target IDs (first 5): [275, 2000, 263, 2802, 331]\n",
      "\n",
      "================ Sample 90 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "\n",
      "finite-dimensional spaces, e.g., see \\cite{Todd76} for a nice survey.\n",
      "In Section \\ref{sec:fixed-dio}, we show how to use Sperner's lemma for\n",
      "a fixed point theorem in the infinite-dimensional space $\\ensuremath{\\Delta^\\infty_0} $.\n",
      "\n",
      "Another crucial theorem for our purposes states that, under\n",
      "appropriate hypotheses, the existence of approximate fixed points\n",
      "implies the existence of fixed points.  On the metric space $(X,d)$,\n",
      "we can quantify the notion of an approximate fixed point by  \n",
      "defining an \\textit{$\\epsilon$-fixed point}, which for a given function\n",
      "$f$ is a point $x\\in X$ such that $d(x,f(x))<\\epsilon$.   \n",
      "Versions of the following lemma may be found in, e.g., \\cite{DuGr82, Smar74}.\n",
      "\n",
      "\\begin{lemma}[Epsilon Fixed Point Theorem] \n",
      "\\label{le epsilonfixed}\n",
      " Suppose that $A$ is a\n",
      "  compact subset of the metric space $(X,d)$ and that $f:A\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [13, 18925, 29899, 12531, 8162]\n",
      "Target IDs (first 5): [18925, 29899, 12531, 8162, 29892]\n",
      "\n",
      "================ Sample 91 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "\\rightarrow\n",
      "  A$ is continuous.   If $f$ has an $\\epsilon$-fixed point for every\n",
      "  $\\epsilon > 0$ then $f$ has a fixed point. \n",
      "\\end{lemma}\n",
      "\n",
      "\\begin{proof} Let $\\{a_n\\}$ be a sequence of $1/n$-fixed points.\n",
      "  That is, $d(a_n,f(a_n)) < 1/n$ for all $n$.   Since $A$ is compact\n",
      "  it is sequentially compact and thus $\\{a_n\\}$ has a convergent\n",
      "  subsequence, which we denote $\\{a'_n\\}$ with $a_n'\\rightarrow x \\in\n",
      "  A$.   Let $\\epsilon >0$.   Since $a_n'\\rightarrow x$ there exists\n",
      "  $N_1$ such that $n\\geq N_1$ implies that $d(a'_n,x) < \\epsilon /2$.\n",
      "  Let $N = \\max ( N_1, 2/\\epsilon)$.   Then $n\\geq N$ implies that\n",
      "\\[\n",
      "d(x, f(a'_n)) \\leq d(x,a'_\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [29905, 5211, 13, 29871, 319]\n",
      "Target IDs (first 5): [5211, 13, 29871, 319, 29938]\n",
      "\n",
      "================ Sample 92 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "n) + d(a'_n,f(a'_n)) < \\epsilon,\n",
      "\\]\n",
      "so that $f(a'_n) \\rightarrow x$.   However, since $f$ is continuous,\n",
      "we also know that $f(a'_n)\\rightarrow f(x)$.   Since limits are\n",
      "unique, we conclude that $f(x)=x$, which completes the\n",
      "proof. \n",
      "\\end{proof}\n",
      "\n",
      "Later it will be desirable to have an isometry between\n",
      "$\\Delta^{n-1}$, the standard $(n-1)$-simplex in $\\ensuremath{\\mathbb{R}} ^{n}$, and\n",
      "$F^{n-1}$.  The easiest way to do this is to consider\n",
      "$\\ensuremath{\\mathbb{R}} ^n$ as a subspace of $\\ensuremath{\\mathbb{R}} ^\\infty$ by projection onto the first $n$\n",
      "factors, and restricting the metric on $\\ensuremath{\\mathbb{R}} ^\\infty$ to $\\ensuremath{\\mathbb{R}} ^{n}$.  Call\n",
      "this metric $\\bar d_{n}$ and consider $\\Delta^{n-1}$ \n",
      "in the metric space $(\\ensuremath{\\mathbb{R}} ^{\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [29876, 29897, 718, 270, 29898]\n",
      "Target IDs (first 5): [29897, 718, 270, 29898, 29874]\n",
      "\n",
      "================ Sample 93 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "n}, \\bar d_{n})$.\n",
      "It is worthwhile to ensure that \n",
      "$(\\ensuremath{\\mathbb{R}} ^{n}, \\bar d_n)$ has a rich supply of continuous functions.  \n",
      "Before proceeding, recall that all norms on $\\ensuremath{\\mathbb{R}} ^n$ are equivalent \n",
      "and thus essentially interchangeable; we now prove that $\\bar d_n$ is\n",
      "interchangeable with norm-induced metrics on bounded sets.\n",
      "\n",
      "\\begin{lemma} \n",
      "\\label{le metricequivalence}\n",
      "Let $A$ be a bounded subset of the normed space $(\\ensuremath{\\mathbb{R}} ^{n}\n",
      "  , \\| \\cdot \\|_\\infty)$.   On $A$, the metric $\\bar d_n$ is\n",
      "  equivalent to the metric induced by the norm $\\| \\cdot \\|_\\infty$.\n",
      "\\end{lemma}\n",
      "\n",
      "\\begin{proof} Suppose that $x,y \\in \\ensuremath{\\mathbb{R}} ^n$.   We see that \n",
      "\\[\n",
      "\\bar d_n(x,y) = \\sum_{i=1}^n \\frac{|x_i - y_i|}{\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [29876, 1118, 320, 1646, 270]\n",
      "Target IDs (first 5): [1118, 320, 1646, 270, 648]\n",
      "\n",
      "================ Sample 94 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "2^i(1+|x_i-y_i|)} \\leq\n",
      "n\\|x-y\\|_\\infty.\n",
      "\\]\n",
      "Now, since $A$ is bounded, there\n",
      "is some $M$ such $\\|x-y\\|_\\infty \\leq M$ for $x,y \\in A$.   Thus we\n",
      "see that\n",
      "\\[\\frac{\\|x-y\\|_\\infty}{2^n(1+M)} \\leq\n",
      "\\frac{\\|x-y\\|_\\infty}{2^n(1+\\|x-y\\|_\\infty)} \\leq \\bar d_n(x,y),\n",
      "\\]\n",
      "which implies that \n",
      "\\begin{equation}\n",
      "  \\label{eq:norm-bound}\n",
      "\\|x-y\\|_\\infty \\leq 2^n(1+M)\\bar d_n(x,y).\n",
      "\\end{equation}\n",
      "Thus\n",
      "$\\bar d_n$ is equivalent to the metric induced by the norm on\n",
      "$A$.\\end{proof} \n",
      "\n",
      "Lemma \\ref{le metricequivalence} tells us that bounded subsets of $\\ensuremath{\\mathbb{R}} ^n$ have\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [29906, 29985, 29875, 29898, 29896]\n",
      "Target IDs (first 5): [29985, 29875, 29898, 29896, 29974]\n",
      "\n",
      "================ Sample 95 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "\n",
      "the same continuous functions regardless of whether they are\n",
      "considered as subsets of a normed space or as subsets of $(\\ensuremath{\\mathbb{R}} ^n , \\bar\n",
      "d_n)$.  Importantly, notice that $\\Delta^{n-1}$ is bounded.\n",
      "Furthermore, the isometry \n",
      "$f:\\Delta^{n-1} \\rightarrow F^{n-1}$\n",
      "between $\\Delta^{n-1}$ in $(\\ensuremath{\\mathbb{R}} ^n,\\bar d_n)$\n",
      "and $F^{n-1}$ in $\\ensuremath{\\mathbb{R}} ^\\infty$ is clearly given\n",
      "by $f(x) = f(x_1,x_2,\\dots,x_n) =\n",
      "(x_1,x_2,\\dots,x_n,0,0,\\dots)$.  \n",
      "This is important because it implies that $F^{n-1}$ has an arbitrarily\n",
      "small barycentric subdivision.\n",
      "Recall that the diameter of a set $X$ is\n",
      "$d(X) = \\sup_{x,y\\in X} d(x,y)$ and if $\\mathscr T$ is a\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [13, 1552, 1021, 9126, 3168]\n",
      "Target IDs (first 5): [1552, 1021, 9126, 3168, 17126]\n",
      "\n",
      "================ Sample 96 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "family of\n",
      "sets, then $size(\\mathscr T) = \\sup_{\\sigma \\in \\mathscr T}\n",
      "d(\\sigma)$.\n",
      "Thus, given $\\epsilon >0$, $F^{n-1}$ has a barycentric subdivision $\\mathscr\n",
      "T$ with $size(\\mathscr T) < \\epsilon$. \n",
      "\n",
      "\\begin{comment}****************\n",
      "\\begin{proof} As proved in (include reference number) the standard\n",
      "  $(n-1)$-simplex in the normed space $\\ensuremath{\\mathbb{R}} ^n$ as an\n",
      "  arbitrarily small barycentric subdivision, and thus Lemma 2 tells us\n",
      "  that the standard $(n-1)$-simplex in the metric space\n",
      "  $(\\ensuremath{\\mathbb{R}} ^n,\\bar d_n)$ has an arbitrarily fine subdivision.   It is clear\n",
      "  that this subdivision is preserved by the isometry between $F^{n-1}$\n",
      "  and $\\Delta^{n-1}$. \n",
      "\\end{proof}\n",
      "\\end{comment}************************\n",
      "\n",
      "Now we are ready to prove a fixed point theorem for $\\ensuremath{\\Delta^\\infty_0} $.\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [3942, 310, 13, 7224, 29892]\n",
      "Target IDs (first 5): [310, 13, 7224, 29892, 769]\n",
      "\n",
      "================ Sample 97 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "\n",
      "\n",
      "\n",
      "\\section{A Fixed Point Theorem for $\\ensuremath{\\Delta^\\infty_0} $}\n",
      "\\label{sec:fixed-dio}\n",
      "\n",
      "\\begin{theorem}\n",
      "\\label{fixed-dio}\n",
      "Suppose that $f:\\ensuremath{\\Delta^\\infty_0} \\rightarrow \\ensuremath{\\Delta^\\infty_0} $ is continuous.   Then $f$ has a\n",
      "fixed point. \n",
      "\\end{theorem} \n",
      "\n",
      "\\begin{proof} \n",
      "Since $\\ensuremath{\\Delta^\\infty_0} $ is compact, by Lemma \\ref{le epsilonfixed}, it is sufficient to show that $f$\n",
      "has an $\\epsilon$-fixed point for each $\\epsilon >0$.   Let $\\epsilon\n",
      ">0$ be given.   Choose $N \\geq \\log_2(2/\\epsilon)+1$.   Notice that for\n",
      "$x,y \\in \\ensuremath{\\Delta^\\infty_0} $, this implies that \n",
      "\\begin{equation}\n",
      "\\label{eq:eps-over-2-N+1on}\n",
      "\\sum_{i=N+1}^\\infty   \\frac{|x_i - y_i|}{\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [13, 13, 13, 29905, 2042]\n",
      "Target IDs (first 5): [13, 13, 29905, 2042, 29912]\n",
      "\n",
      "================ Sample 98 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "2^i(1+|x_i-y_i|)}  \\leq\n",
      "\\sum_{i=N+1}^\\infty \\frac{1}{2^i} < \\frac{\\epsilon}{2}.\n",
      "\\end{equation}\n",
      "Since $f$ maps between countably infinite-dimensional spaces, \n",
      "we can write $f$ in terms of its components: \n",
      "$f(x) = (f_1(x),f_2(x),\\dots)$.   \n",
      "Since $f$ is continuous, \n",
      "$f_i$ is continuous for each $i$.   Consider the function \n",
      "\\[\n",
      "g(x) =(g_1(x),g_2(x),\\dots) =  (f_1(x),f_2(x),\\dots, f_N(x) ,\n",
      "1-\\sum_{i=1}^N f_i(x), 0,0,0,\\dots ).\n",
      "\\]   \n",
      "Since each $f_i$ is continuous and finite sums of continuous function\n",
      "are continuous, $g_i$ is continuous for each $i$.   Furthermore, we\n",
      "see that $g:F^N \\rightarrow F\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [29906, 29985, 29875, 29898, 29896]\n",
      "Target IDs (first 5): [29985, 29875, 29898, 29896, 29974]\n",
      "\n",
      "================ Sample 99 ================\n",
      "Shapes -> input_ids: torch.Size([256]), targets: torch.Size([256])\n",
      "\n",
      "[Decoded Text Preview]:\n",
      "^N$.   Consequently, $g$ is continuous.    \n",
      "\n",
      "Let $\\epsilon_0 = \\frac{\\epsilon}{8(N+1)}$ and $\\epsilon_1 =\n",
      "\\frac{\\epsilon}{2^{N+5}(N+1)}$.   Since $g$ is continuous on a compact\n",
      "set, it is uniformly continuous.   Thus there exists $\\delta_1 >0$\n",
      "such that $\\bar d(x,y) < \\delta_1$ implies that $\\bar d(g(x),g(y)) <\n",
      "\\epsilon_1$.   Let $\\delta = \\min ( \\delta_1 , \\epsilon_1)$.   Since\n",
      "$F^N$ can be triangulated with an arbitrarily small triangulation, let\n",
      "$\\mathscr T$ be a triangulation with $size(\\mathscr T) < \\delta$.   \n",
      "Label the vertices of $\\mathscr T$ with the map \n",
      "\\[\n",
      "\\ell(x) = \\mbox{argmax}_i (x_i - g_i(x)).\n",
      "\\]\n",
      "Recall that the \\emph{argmax} function returns the index of the\n",
      "largest element of the argument, and if\n",
      "\n",
      "[Shift Verification]:\n",
      "Input IDs  (first 5): [29985, 29940, 1504, 259, 1281]\n",
      "Target IDs (first 5): [29940, 1504, 259, 1281, 27284]\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = load_tokenizer(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "\n",
    "# dataset = TokenizedBinaryDataset(\n",
    "#     source=\"/home/prasanna/.cache/huggingface/datasets/generator/default-caaf09e7aefe9c5f/0.0.0/\",\n",
    "#     tokenizer=tokenizer,\n",
    "#     block_size=256,\n",
    "#     bin_path=\"./data/automathtext.bin\",\n",
    "# )\n",
    "\n",
    "# # Call the inspect method directly!\n",
    "# dataset.inspect(tokenizer=tokenizer, num_samples=100, check_duplicates=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23c7e18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load_tokenizer(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "\n",
    "dataset = PretrainDataset(\n",
    "    data_dir=\"data/AutoMathText\",\n",
    "    tokenizer=tokenizer,\n",
    "    block_size=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e8d4c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['url', 'title', 'abstract', 'text', 'meta'],\n",
      "    num_rows: 45169\n",
      "})\n",
      "rows: 45169\n",
      "columns: ['url', 'title', 'abstract', 'text', 'meta']\n",
      "\n",
      "row[0] keys: dict_keys(['url', 'title', 'abstract', 'text', 'meta'])\n",
      "title: Convergence directions of the randomized Gauss--Seidel method and its extension\n",
      "abstract: The randomized Gauss--Seidel method and its extension have attracted much attention recently and their convergence rates have been considered extensively. However, the convergence rates are usually de\n",
      "text: \\section{Introduction}\n",
      "Linear least squares problem is a ubiquitous problem arising frequently in data analysis and scientific computing. Specifically, given a data matrix $A\\in R^{m\\times n}$ and a data vector $b\\in R^{m}$, a linear least squares problem can be written as follows\n",
      "\\begin{equation}\n",
      "\\label{ls}\n",
      "\\min \\limits _{ x \\in R^{n}}\\|b-Ax\\|^2_{2}.\n",
      "\\end{equation}\n",
      "In the literature, several direct methods have been proposed for solving its normal equations $A^TAx=A^Tb$ through either the QR fa\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "ds = load_from_disk(\"data/AutoMathText\")  # change path if needed\n",
    "print(ds)\n",
    "print(\"rows:\", len(ds))\n",
    "print(\"columns:\", ds.column_names)\n",
    "\n",
    "# first row raw\n",
    "row0 = ds[0]\n",
    "print(\"\\nrow[0] keys:\", row0.keys())\n",
    "print(\"title:\", (row0.get(\"title\") or \"\")[:120])\n",
    "print(\"abstract:\", (row0.get(\"abstract\") or \"\")[:200])\n",
    "print(\"text:\", (row0.get(\"text\") or \"\")[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c727a9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence directions of the randomized Gauss--Seidel method and its extension\n",
      "\n",
      "The randomized Gauss--Seidel method and its extension have attracted much attention recently and their convergence rates have been considered extensively. However, the convergence rates are usually determined by upper bounds, which cannot fully reflect the actual convergence. In this paper, we make a detailed analysis of their convergence behaviors. The analysis shows that the larger the singular value of $A$ is, the faster the error decays in the corresponding singular vector space, and the convergence directions are mainly driven by the large singular values at the beginning, then gradually driven by the small singular values, and finally by the smallest nonzero singular value. These results explain the phenomenon found in the extensive numerical experiments appearing in the literature that these two methods seem to converge faster at the beginning. Numerical examples are provided to confirm the above fi\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Cdatasets.dataset import PretrainDataset\n",
    "from Cdatasets.tokenizer import load_tokenizer\n",
    "\n",
    "tok = load_tokenizer(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "pds = PretrainDataset(\"data/AutoMathText\", tok, block_size=256)\n",
    "\n",
    "# pick raw HF row and format it the same way as training\n",
    "raw = ds[0]\n",
    "formatted = pds._format_row(raw)\n",
    "print(formatted[:1000])\n",
    "tok.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b47377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher: 60.90M parameters\n",
      "Student: 39.69M parameters\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = MathTokenizer()\n",
    "teacher = GQATransformer(\n",
    "    num_layers=tc.num_layers,\n",
    "    n_emb=tc.n_embd,\n",
    "    n_head=tc.n_head,\n",
    "    n_kv_head=tc.n_kv_head,\n",
    "    vocab_size=tc.vocab_size,\n",
    "    block_size=tc.block_size,\n",
    "    dropout=tc.dropout,\n",
    ")\n",
    "\n",
    "student = GQATransformer(\n",
    "    num_layers=sc.num_layers,\n",
    "    n_emb=sc.n_embd,\n",
    "    n_head=sc.n_head,\n",
    "    n_kv_head=sc.n_kv_head,\n",
    "    vocab_size=sc.vocab_size,\n",
    "    block_size=sc.block_size,\n",
    "    dropout=sc.dropout,\n",
    ")\n",
    "\n",
    "\n",
    "def count_parameters(model: GQATransformer) -> int:\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"Teacher: {count_parameters(teacher) / 1e6:.2f}M parameters\")\n",
    "print(f\"Student: {count_parameters(student) / 1e6:.2f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18557135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c19d9319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher: 121.74M parameters\n",
      "Student: 27.51M parameters\n"
     ]
    }
   ],
   "source": [
    "print(f\"Teacher: {count_parameters(teacher) / 1e6:.2f}M parameters\")\n",
    "print(f\"Student: {count_parameters(student) / 1e6:.2f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1458ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepseek_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"deepseek-ai/deepseek-math-7b-base\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "text = r\"\"\"\n",
    "Solve: \\int_0^1 x^2 dx.\n",
    "Chain of thought: First compute the antiderivative...\n",
    "\"\"\"\n",
    "\n",
    "tokens = deepseek_tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "print(\"Token IDs:\", tokens[\"input_ids\"][0][:20])\n",
    "print(\"Decoded:\", deepseek_tokenizer.decode(tokens[\"input_ids\"][0]))\n",
    "print(\"Vocab size:\", deepseek_tokenizer.vocab_size)\n",
    "print(\"Special tokens:\", deepseek_tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4267ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qwen_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-Math-7B\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "text = r\"\"\"\n",
    " a,b \\in \\mathbb{R},  a^2 + b^2 \\ge 2ab.\n",
    "\"\"\"\n",
    "\n",
    "tokens = qwen_tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "print(\"Token IDs:\", tokens[\"input_ids\"][0][:20])\n",
    "print(\"Decoded:\", qwen_tokenizer.decode(tokens[\"input_ids\"][0]))\n",
    "print(\"Vocab size:\", qwen_tokenizer.vocab_size)\n",
    "print(\"Special tokens:\", qwen_tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c138471c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: tensor([    1,   960, 29871, 29896, 29906, 29941, 29946, 29945, 29985, 29906,\n",
      "          353,  1577, 29892, 10272,  4331, 29899,  1609, 29899, 10568, 29889])\n",
      "Decoded: <s> If 12345^2 = ?, compute step-by-step.\n",
      "Vocab size: 32000\n",
      "Special tokens: {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llama_tokenizer = load_tokenizer(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "\n",
    "text = \"If 12345^2 = ?, compute step-by-step.\"\n",
    "\n",
    "tokens = llama_tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "print(\"Token IDs:\", tokens[\"input_ids\"][0][:20])\n",
    "print(\"Decoded:\", llama_tokenizer.decode(tokens[\"input_ids\"][0]))\n",
    "print(\"Vocab size:\", llama_tokenizer.vocab_size)\n",
    "print(\"Special tokens:\", llama_tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60f2f247",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"Hello world? how is it going ? what's up\",\n",
    "    \"How are you today\",\n",
    "    \"This is a small demo\",\n",
    "    \"Transformers are powerful\",\n",
    "    \"Packing saves compute\",\n",
    "    \"Deep learning models\",\n",
    "    \"Tokenization example\",\n",
    "    \"Short text\",\n",
    "    \"Another sentence here\",\n",
    "    \"Final sample\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0318714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\") \n",
    "EOS = tokenizer.eos_token_id \n",
    "BLOCK_SIZE = 24   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c87d212d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77a88109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= PADDING APPROACH =================\n",
      "\n",
      "Text 1: Hello world? how is it going ? what's up\n",
      "Token IDs : [15496, 995, 30, 703, 318, 340, 1016, 5633, 644, 338, 510, 50256, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Decoded   : Hello world? how is it going ? what's up<|endoftext|>\n",
      "\n",
      "Text 2: How are you today\n",
      "Token IDs : [2437, 389, 345, 1909, 50256, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Decoded   : How are you today<|endoftext|>\n",
      "\n",
      "Text 3: This is a small demo\n",
      "Token IDs : [1212, 318, 257, 1402, 13605, 50256, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Decoded   : This is a small demo<|endoftext|>\n",
      "\n",
      "Text 4: Transformers are powerful\n",
      "Token IDs : [41762, 364, 389, 3665, 50256, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Decoded   : Transformers are powerful<|endoftext|>\n",
      "\n",
      "Text 5: Packing saves compute\n",
      "Token IDs : [47, 5430, 16031, 24061, 50256, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Decoded   : Packing saves compute<|endoftext|>\n",
      "\n",
      "Text 6: Deep learning models\n",
      "Token IDs : [29744, 4673, 4981, 50256, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Decoded   : Deep learning models<|endoftext|>\n",
      "\n",
      "Text 7: Tokenization example\n",
      "Token IDs : [30642, 1634, 1672, 50256, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Decoded   : Tokenization example<|endoftext|>\n",
      "\n",
      "Text 8: Short text\n",
      "Token IDs : [16438, 2420, 50256, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Decoded   : Short text<|endoftext|>\n",
      "\n",
      "Text 9: Another sentence here\n",
      "Token IDs : [6610, 6827, 994, 50256, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Decoded   : Another sentence here<|endoftext|>\n",
      "\n",
      "Text 10: Final sample\n",
      "Token IDs : [19006, 6291, 50256, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Decoded   : Final sample<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n================= PADDING APPROACH =================\")\n",
    "\n",
    "padded_batches = []\n",
    "\n",
    "for text in texts:\n",
    "    tokens = tokenizer.encode(text) + [EOS]\n",
    "    padded = tokens + [0] * (BLOCK_SIZE - len(tokens))\n",
    "    padded = padded[:BLOCK_SIZE]\n",
    "    padded_batches.append(padded)\n",
    "\n",
    "for i, batch in enumerate(padded_batches):\n",
    "    print(f\"\\nText {i+1}: {texts[i]}\")\n",
    "    print(\"Token IDs :\", batch)\n",
    "    print(\"Decoded   :\", tokenizer.decode([t for t in batch if t != 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e21c26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= PACKING APPROACH =================\n",
      "\n",
      "Concatenated Token Stream:\n",
      "[15496, 995, 30, 703, 318, 340, 1016, 5633, 644, 338, 510, 50256, 2437, 389, 345, 1909, 50256, 1212, 318, 257, 1402, 13605, 50256, 41762, 364, 389, 3665, 50256, 47, 5430, 16031, 24061, 50256, 29744, 4673, 4981, 50256, 30642, 1634, 1672, 50256, 16438, 2420, 50256, 6610, 6827, 994, 50256, 19006, 6291, 50256]\n",
      "\n",
      "Packed Chunk 1:\n",
      "Token IDs : [15496, 995, 30, 703, 318, 340, 1016, 5633, 644, 338, 510, 50256, 2437, 389, 345, 1909, 50256, 1212, 318, 257, 1402, 13605, 50256, 41762]\n",
      "Decoded   : Hello world? how is it going ? what's up<|endoftext|>How are you today<|endoftext|>This is a small demo<|endoftext|>Transform\n",
      "\n",
      "Packed Chunk 2:\n",
      "Token IDs : [364, 389, 3665, 50256, 47, 5430, 16031, 24061, 50256, 29744, 4673, 4981, 50256, 30642, 1634, 1672, 50256, 16438, 2420, 50256, 6610, 6827, 994, 50256]\n",
      "Decoded   : ers are powerful<|endoftext|>Packing saves compute<|endoftext|>Deep learning models<|endoftext|>Tokenization example<|endoftext|>Short text<|endoftext|>Another sentence here<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n================= PACKING APPROACH =================\")\n",
    "\n",
    "all_tokens = []\n",
    "\n",
    "for text in texts:\n",
    "    tokens = tokenizer.encode(text) + [EOS]\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "print(\"\\nConcatenated Token Stream:\")\n",
    "print(all_tokens)\n",
    "\n",
    "packed_batches = []\n",
    "for i in range(0, len(all_tokens), BLOCK_SIZE):\n",
    "    chunk = all_tokens[i:i + BLOCK_SIZE]\n",
    "    if len(chunk) == BLOCK_SIZE:\n",
    "        packed_batches.append(chunk)\n",
    "\n",
    "for i, chunk in enumerate(packed_batches):\n",
    "    print(f\"\\nPacked Chunk {i+1}:\")\n",
    "    print(\"Token IDs :\", chunk)\n",
    "    print(\"Decoded   :\", tokenizer.decode(chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f53be703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padding total tokens: 240\n",
      "Packing total tokens: 48\n",
      "Actual tokens: 51\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPadding total tokens:\", len(padded_batches) * BLOCK_SIZE)\n",
    "print(\"Packing total tokens:\", len(packed_batches) * BLOCK_SIZE)\n",
    "print(\"Actual tokens:\", len(all_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ece4a3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Transform', 'ers']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens([41762, 364]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
